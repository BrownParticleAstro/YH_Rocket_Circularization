{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbPFtYBYafdaexOo/Stpbz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"adl7l9LuadI9"},"outputs":[],"source":["import torch\n","import numpy as np\n","\n","import gym\n","import itertools\n","import statistics\n","import statsmodels.api as sm\n","from itertools import accumulate\n","\n","import pprint\n","import plotly.io as pio\n","import plotly.express as px\n","import plotly.graph_objs as go\n","\n","import os\n","import glob\n","import time\n","import concurrent.futures\n","from datetime import datetime\n","\n","import torch\n","import numpy as np\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib.animation import FuncAnimation\n","from matplotlib.patches import FancyArrowPatch as Arrow\n","from typing import Any, List, Sequence, Tuple\n","\n","import math\n","import tensorflow as tf\n","from typing import Optional, Union\n","\n","import gym\n","from gym.spaces import Box\n","from gym.utils import seeding\n","from gym import logger, spaces\n","from gym.envs.classic_control import utils\n","from gym.error import DependencyNotInstalled\n","\n","import torch\n","import torch.nn as nn\n","from torch.distributions import MultivariateNormal\n","from torch.distributions import Categorical\n","\n","from IPython.display import HTML\n","from base64 import b64encode\n","from typing import Any, List, Sequence, Tuple\n","from logging import raiseExceptions\n","from typing import (\n","    Any, Generic, Iterable, List, Mapping, Optional, Sequence, Tuple, Type,\n","    TypeVar, Union, Dict, Callable\n",")\n","\n","class RocketAnimation(object):\n","    def __init__(self, r_min=0.1, r_target=1, r_max=10, xlim=(-10.2, 10.2), ylim=(-10.2, 10.2), markersize=10, circle_alpha=1, t_vec_len=1):\n","        self.r_min = r_min\n","        self.r_target = r_target\n","        self.r_max = r_max\n","\n","        self.marker_size = markersize\n","        self.circle_alpha = circle_alpha\n","        self.t_vec_len = t_vec_len\n","\n","        self.states = list()\n","        self.thrusts = list()\n","        self.requested_thrusts = list()\n","        self.theta_dot = list()\n","\n","        self.rewards = list()\n","        self.action = list()\n","\n","        self.rmin = list()\n","        self.rtarget = list()\n","        self.rmax = list()\n","\n","        self.xlim = xlim\n","        self.ylim = ylim\n","\n","    def _circle(self, radius):\n","        theta = np.linspace(0, 2 * np.pi, 100)\n","        x, y = radius * np.cos(theta), radius * np.sin(theta)\n","        return x, y\n","\n","    def _init(self,):\n","        self.ax.clear()\n","\n","        self.arrow = Arrow(posA=(0, 0), posB=(0, 0), arrowstyle='simple', mutation_scale=5, color='r')\n","        self.ax.add_patch(self.arrow)\n","        self.line, = self.ax.plot([], [], marker='o', markersize=self.marker_size, alpha=self.circle_alpha)\n","        self.min_circle, = self.ax.plot(*self._circle(self.r_min), '--', label='Minimum Radius')\n","        #self.start_circle, = self.ax.plot(*self._circle(self.r_start), '--', label='Starting Orbit', color='tab:orange') ##### TARGET = 0.3\n","        self.target_circle, = self.ax.plot(*self._circle(self.r_target), '--', label='Target Orbit')\n","        self.max_circle, = self.ax.plot(*self._circle(self.r_max), '--', label='Maximum Radius')\n","        self.ax.grid(True)\n","        self.ax.legend(loc='upper left')\n","        return self.line, self.min_circle, self.target_circle, self.max_circle\n","\n","    def _animate(self, i):\n","        st = self.states[i]\n","        vec = self.thrusts[i] * self.t_vec_len * (self.xlim[1] - self.xlim[0])\n","\n","        self.line.set_data([st[0]], [st[1]])\n","        self.min_circle.set_data(*self._circle(self.rmin[i]))\n","        self.target_circle.set_data(*self._circle(self.rtarget[i]))\n","        self.max_circle.set_data(*self._circle(self.rmax[i]))\n","\n","        self.arrow.set_positions(posA=st[:2], posB=st[:2] + vec)\n","        self.fig.suptitle(f'Iteration: {i}')\n","        return self.line, self.min_circle, self.target_circle, self.max_circle\n","\n","    def save_animation(self, name):\n","        self._transform_vectors()\n","        self.fig = plt.figure(figsize=(10, 10), num=1,\n","                              clear=True, tight_layout=True)\n","        self.ax = self.fig.add_subplot(111)\n","        anim = FuncAnimation(self.fig, self._animate, init_func=self._init, frames=len(\n","            self.states), blit=True, interval=100, repeat=False)\n","        anim.save(name, dpi=80)\n","\n","    def summary_plot(self):\n","        self._transform_vectors()\n","        self.fig1 = plt.figure(figsize=(10, 12), num=1,\n","                              clear=True, tight_layout=True)\n","        ax1 = self.fig1.add_subplot(611)\n","        ax2 = self.fig1.add_subplot(612)\n","        ax3 = self.fig1.add_subplot(613)\n","        ax4 = self.fig1.add_subplot(614)\n","        ax5 = self.fig1.add_subplot(615)\n","        ax6 = self.fig1.add_subplot(616)\n","\n","        self.fig1.suptitle('Run Summary')\n","\n","        # Plot Thrust Value\n","        ax1.set_title('Thrust Values')\n","        ax1.plot([thrust[1] for thrust in self.thrusts_polar], label='thrust tangent')\n","        ax1.grid(True)\n","        ax1.legend()\n","\n","        # Plot Theta Dot\n","        ax2.set_title('theta_dot')\n","        ax2.plot([theta for theta in self.theta_dot], label='theta_dot')\n","        ax2.grid(True)\n","        ax2.legend()\n","\n","        # Plot Radius\n","        ax3.set_title('Radius')\n","        ax3.plot(self.rs, label='radius')\n","        ax3.grid(True)\n","        ax3.legend()\n","\n","        # Plot Velocities\n","        ax4.set_title('Radial Velocities')\n","        ax4.plot([vel[0] for vel in self.vel_polar], label='radial velocity')\n","        ax4.grid(True)\n","        ax4.legend()\n","\n","        # Tangential Velocities\n","        ax5.set_title(\"Tangential Velocities\")\n","        ax5.plot([vel[1] for vel in self.vel_polar], label='tangential velocity')\n","        ax5.grid(True)\n","        ax5.legend()\n","\n","        # Plot Cumulative Thrust\n","        ax6.set_title(\"Cumulative Thrust\")\n","        ax6.plot(list(accumulate(self.thrusts_polar, lambda x, thrust: x + thrust[0], initial=0))[1:], label='cumulative thrust')\n","        ax6.grid(True)\n","        ax6.legend()\n","\n","        self.fig1.tight_layout()\n","        return self.fig1\n","\n","    def get_run_info(self):\n","      self._transform_vectors()\n","      return [[thrust[1] for thrust in self.thrusts_polar],\n","              [theta for theta in self.theta_dot],\n","              self.rs,\n","              [vel[0] for vel in self.vel_polar],\n","              [vel[1] for vel in self.vel_polar]]\n","\n","    def _get_transforms(self, states):\n","        transforms = list()\n","        rs = list()\n","        thetas = list()\n","\n","        for st in states:\n","            pos, vel = st[:2], st[2:]\n","            r = np.linalg.norm(pos)\n","            theta = np.arctan2(pos[1], pos[0])\n","            rhat = pos / r\n","\n","            rot_mat = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","            transforms.append(rot_mat)\n","            rs.append(r)\n","            thetas.append(theta)\n","\n","        return transforms, rs, thetas\n","\n","    def _forward_transform(self, transforms, vecs):\n","        return [tr @ vec for tr, vec in zip(transforms, vecs)]\n","\n","    def _inverse_transform(self, transforms, vecs):\n","        return [tr.T @ vec for tr, vec in zip(transforms, vecs)]\n","\n","\n","    def _transform_vectors(self, ):\n","        transforms, self.rs, self.thetas = self._get_transforms(self.states)\n","        self.vel_polar = self._inverse_transform(\n","            transforms, [st[2:] for st in self.states])\n","        self.thrusts_polar = self._inverse_transform(transforms, self.thrusts)\n","        self.requested_thrusts_polar = self._inverse_transform(\n","            transforms, self.requested_thrusts)\n","        self.theta_dot = [v_tangential / r if r != 0 else 0 for v_tangential, r in zip([vel[1] for vel in self.vel_polar], self.rs)]\n","        self.thrusts_norm = [np.linalg.norm(thrust) for thrust in self.thrusts]\n","        self.requested_thrusts_norm = [np.linalg.norm(\n","            thrust) for thrust in self.requested_thrusts]\n","        self.thrust_direction = [np.arctan2(\n","            thrust[1], thrust[0]) for thrust in self.thrusts_polar]\n","        self.requested_thrust_direction = [np.arctan2(\n","            thrust[1], thrust[0]) for thrust in self.requested_thrusts_polar]\n","\n","    def render(self, state, thrust, requested_thrust, rmin, rtarget, rmax):\n","        self.states.append(state)\n","        self.thrusts.append(thrust)\n","        self.requested_thrusts.append(requested_thrust)\n","        self.rmin.append(rmin)\n","        self.rtarget.append(rtarget)\n","        self.rmax.append(rmax)\n","\n","    def return_arrays(self):\n","      return self.states, self.thrusts\n","\n","def show_video(video_path, video_width = 600):\n","  video_file = open(video_path, \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n","\n","device = torch.device('cpu')\n","if(torch.cuda.is_available()):\n","    device = torch.device('cuda:0')\n","    torch.cuda.empty_cache()\n","    print(\"Device set to : \" + str(torch.cuda.get_device_name(device)))\n","else:\n","    print(\"Device set to : cpu\")\n","\n","\n","################################## PPO Policy ##################################\n","class RolloutBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.state_values = []\n","        self.is_terminals = []\n","        self.ep_done = []\n","\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.state_values[:]\n","        del self.is_terminals[:]\n","        del self.ep_done[:]\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, has_continuous_action_space, action_std_init):\n","        super(ActorCritic, self).__init__()\n","        #In this section I'm only interested in the continous space\n","        self.has_continuous_action_space = has_continuous_action_space\n","\n","        if has_continuous_action_space:\n","            self.action_dim = action_dim\n","            self.action_var = torch.full((action_dim,), action_std_init * action_std_init).to(device)\n","\n","        # actor\n","        #I'm mainly going to keep the same dimensions as the ones used above\n","        if has_continuous_action_space :\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, action_dim),\n","                            nn.Tanh()\n","                        )\n","        else:\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.LeakyReLU(),\n","                            nn.Linear(12, 12),\n","                            nn.Tanh(),\n","                            nn.Linear(12, action_dim),\n","                            nn.Softmax(dim=-1)\n","                        )\n","        # critic\n","        self.critic = nn.Sequential(\n","                        nn.Linear(state_dim, 12),\n","                        nn.Tanh(),\n","                        nn.Linear(12, 12),\n","                        nn.Tanh(),\n","                        nn.Linear(12, 12),\n","                        nn.Tanh(),\n","                        nn.Linear(12, 12),\n","                        nn.Tanh(),\n","                        nn.Linear(12, 1)\n","                    )\n","\n","\n","    def set_action_std(self, new_action_std):\n","        if self.has_continuous_action_space:\n","            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std).to(device)\n","\n","    # This function is used during the interaction phase, where the agent interacts with the environment to collect experiences.\n","    def act(self, state, min, max):\n","        # the continous action\n","        if self.has_continuous_action_space:\n","          #so the actor has as a purpose to give off the meand of the action explaining even more the use of tanh\n","            action_mean = self.actor(state)\n","            # so that's where we aff the varience to it\n","            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n","            # This creqtes the multivavariate normal distribution with the specified mean and cov_mat\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","        # It samples from the distribution the action\n","        action = dist.sample()\n","        #print(\"action before clamp\",action)\n","        #action = torch.clip(action,min,max) #ensures that the action is between a specific range.\n","        #print(\"action after clamp\", action)\n","        action_logprob = dist.log_prob(action)\n","        state_val = self.critic(state)\n","\n","        return action.detach(), action_logprob.detach(), state_val.detach()\n","\n","  # This function is used during the training phase\n","    def evaluate(self, state, action):\n","\n","        if self.has_continuous_action_space:\n","\n","            action_mean = self.actor(state)\n","            action_var = self.action_var.expand_as(action_mean)\n","            cov_mat = torch.diag_embed(action_var).to(device)\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","\n","            # For Single Action Environments.(which is our situation here)\n","            if self.action_dim == 1:\n","                action = action.reshape(-1, self.action_dim)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        state_values = self.critic(state)\n","\n","        return action_logprobs, state_values, dist_entropy\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, has_continuous_action_space=True, action_std_init=0.6):\n","\n","        self.has_continuous_action_space = has_continuous_action_space\n","\n","        if has_continuous_action_space:\n","            self.action_std = action_std_init\n","\n","        self.gamma = gamma # discount factor\n","        self.eps_clip = eps_clip\n","        self.K_epochs = K_epochs\n","        self.buffer = RolloutBuffer()\n","        self.policy = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device)\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n","                        {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n","                    ]) #specific learning rate for the actor and for the critic\n","\n","        self.policy_old = ActorCritic(state_dim, action_dim, has_continuous_action_space, action_std_init).to(device) #I'm wondering if this line is actually necessary\n","        self.policy_old.load_state_dict(self.policy.state_dict()) # apparently that copies the weights of the policy dict so that it starts at the same point, before any update is being made.\n","\n","        self.MseLoss = nn.MSELoss()\n","\n","    def set_action_std(self, new_action_std):\n","        if self.has_continuous_action_space:\n","            self.action_std = new_action_std\n","            self.policy.set_action_std(new_action_std)\n","            self.policy_old.set_action_std(new_action_std)\n","\n","#we decay the standard deviation in order to have a more norrow down possible actions to take so more exploitation and less exploration.\n","    def decay_action_std(self, action_std_decay_rate, min_action_std):\n","        if self.has_continuous_action_space:\n","            self.action_std = self.action_std - action_std_decay_rate\n","            self.action_std = round(self.action_std, 4)\n","            if (self.action_std <= min_action_std):\n","                self.action_std = min_action_std\n","            self.set_action_std(self.action_std)\n","\n","    def select_action(self, state, min , max):\n","        #print(\"It's getting there\")\n","        if self.has_continuous_action_space:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state).to(device)\n","                action, action_logprob, state_val = self.policy_old.act(state, min, max)\n","\n","            self.buffer.states.append(state)\n","            self.buffer.actions.append(action)\n","            self.buffer.logprobs.append(action_logprob)\n","            self.buffer.state_values.append(state_val)\n","\n","            return action.detach().cpu().numpy().flatten()\n","        else:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state).to(device)\n","                action, action_logprob, state_val = self.policy_old.act(state)\n","\n","            self.buffer.states.append(state)\n","            self.buffer.actions.append(action)\n","            self.buffer.logprobs.append(action_logprob)\n","            self.buffer.state_values.append(state_val)\n","\n","            return action.item()\n","\n","    def update(self):\n","        '''\n","        The update function, called when update_timestep is reached and the episode is done\n","        It starts by calculating the discounted reward and uses a geometric sum if the episode ended because it reached the maximum timestep\n","        The normalisation also had been deleted as the number it got normalized to were somehow questionable\n","        '''\n","        good_episode = False # this boolean helps determining if the episode ended because it reached the maximum number of steps\n","        good_episode_reward = 0 # this allows to substract the right reward every time\n","        rewards = [] # discounted reward array\n","        discounted_reward = 0\n","\n","        for reward, is_terminal, ep_done in zip(reversed(self.buffer.rewards), reversed(self.buffer.is_terminals), reversed(self.buffer.ep_done)): #This is basically doing with a for loop what my discount function was doing\n","\n","            if is_terminal: # that will be true no matter how the episode ended when an episode ended\n","                discounted_reward = 0 # reseting the discounted reward\n","                good_episode = False # reseting the boolean to false\n","\n","            if ep_done : # that will be true if the episode ended because it reached the maximum number of stps\n","                discouted_reward = reward * (self.gamma**800 - 1)/(self.gamma -1) # geometric serie\n","                good_episode_reward = reward\n","                good_episode = True\n","\n","            if good_episode : # it will be true for the entirety of the \"good episode\"\n","              discounted_reward = reward + (self.gamma* discounted_reward) - good_episode_reward*self.gamma**800\n","\n","\n","            else : # the episode ended because it reached a bound\n","              discounted_reward = reward + (self.gamma * discounted_reward) # how it was originaly computed !\n","\n","            rewards.insert(0, discounted_reward)\n","\n","        ############################ END OF THE MAIN CHANGES ###################\n","\n","        # Normalizing the rewards (LET'S TRY THAT )\n","        #print(\"at the end of the loop the discounted reward array was :\",rewards)\n","        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","\n","        # convert list to tensor\n","        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach().to(device)\n","        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach().to(device)\n","        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach().to(device)\n","        old_state_values = torch.squeeze(torch.stack(self.buffer.state_values, dim=0)).detach().to(device)\n","\n","        # calculate advantages\n","        advantages = rewards.detach() - old_state_values.detach()\n","\n","        # Optimize policy for K epochs\n","        for _ in range(self.K_epochs):\n","\n","            # Evaluating old actions and values\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions) # I'm confused because I don't really see where we precviously update the policy ...\n","\n","            # match state_values tensor dimensions with rewards tensor\n","            state_values = torch.squeeze(state_values)\n","\n","            # Finding the ratio (pi_theta / pi_theta__old)\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","\n","            # Finding Surrogate Loss\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","\n","            # final loss of clipped objective PPO\n","            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n","\n","            # take gradient step\n","            self.optimizer.zero_grad() # In the initialization we're linking the optimizer to self.policy (update comes partially from here)\n","            loss.mean().backward()\n","            self.optimizer.step()\n","\n","        # Copy new weights into old policy\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","\n","        # clear buffer\n","        self.buffer.clear()\n","\n","\n","    def save(self, checkpoint_path):\n","        torch.save(self.policy_old.state_dict(), checkpoint_path)\n","\n","    def load(self, checkpoint_path):\n","        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","\n","## Adding a continous episode option\n","def make(name):\n","    if name == 'RocketCircularization-v0':\n","        init_func = varied_l(r_min=0.5, r_max=1.5)\n","        return RocketEnv(max_step=400, simulation_step=3, rmax=1.5, rmin=0.5,\n","                         init_func=init_func, max_thrust=.1,\n","                         oob_penalty=0, dt=0.03, wall_mechanics=True,\n","                         velocity_penalty_rate=0.1, thrust_penalty_rate=0.001)\n","    else:\n","        raise ValueError(f'No environment {name}')\n","\n","def uniform(r_min: float = 0.99, r_max: float = 1.01,\n","            rdot_min: float = -0.05, rdot_max: float = 0.05,\n","            thetadot_min: float = 0.99, thetadot_max: float = 1.01) \\\n","        -> Callable[[], List[np.float32]]:\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max, thetadot_min, thetadot_max\n","        r = np.random.uniform(r_min, r_max)\n","        theta = np.random.uniform(0, 2 * np.pi)\n","        rdot = np.random.uniform(rdot_min, rdot_max)\n","        thetadot = np.random.uniform(thetadot_min, thetadot_max)\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","def varied_l(r_min: float = 0.9, r_max: float = 1.1,\n","             rdot_min: float = -0.5, rdot_max: float = 0.5,\n","             dl_min: float = -.5, dl_max: float = .5) \\\n","        -> Callable[[], List[np.float32]]:\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max\n","        r = np.random.uniform(r_min, r_max)\n","        theta = np.random.uniform(0, 2 * np.pi)\n","        rdot = np.random.uniform(rdot_min, rdot_max)\n","        thetadot = (1 + np.random.uniform(dl_min, dl_max)) / r ** 2\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","def target_l(r_min: float = 0.5, r_max: float = 1.5,\n","             rdot_min: float = -0.5, rdot_max: float = 0.5) \\\n","        -> Callable[[], List[np.float32]]:\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max\n","        r = np.random.uniform(r_min, r_max)\n","        theta = np.random.uniform(0, 2 * np.pi)\n","        rdot = np.random.uniform(rdot_min, rdot_max)\n","        thetadot = 1 / r ** 2\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","def stable_at_r11() -> Callable[[], List[np.float32]]:\n","  def func():\n","    r = 1.1\n","    theta = np.random.uniform(0, 2 * np.pi)\n","    rdot = 0\n","    v_dot = 1 / np.sqrt(r)\n","\n","    pos = [r, 0]\n","    vel = [rdot, v_dot]\n","    rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                        [np.sin(theta), np.cos(theta)]])\n","    return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","  return func\n","\n","def stable_at_r1() -> Callable[[], List[np.float32]]:\n","  def func():\n","    r = 1.0\n","    theta = np.random.uniform(0, 2 * np.pi)\n","    rdot = 0\n","    v_dot = 1 / np.sqrt(r)\n","\n","    pos = [r, 0]\n","    vel = [rdot, v_dot]\n","    rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                        [np.sin(theta), np.cos(theta)]])\n","    return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","  return func\n","\n","def stable_orbit_varying_r() -> Callable[[], List[np.float32]]:\n","  def func():\n","    r = np.random.uniform(rmin, rmax, 1)[0]\n","    theta = np.random.uniform(0, 2 * np.pi)\n","    rdot = 0\n","    v_dot = 1 / np.sqrt(r)\n","\n","    pos = [r, 0]\n","    vel = [rdot, v_dot]\n","    rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                        [np.sin(theta), np.cos(theta)]])\n","    return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","  return func\n","\n","# FIRST MODIFICATION : CHANGE OF THE QUADRATIC PENALTY\n","\n","def quadratic_penalty(iters, state: np.ndarray, action: np.ndarray, r_target: float =1, cum_thrust: float = 0,\n","                      w_r: float=1, w_p :float =1,w_t :float =0.5, w_c: float = 0.0001,\n","                      G: float = 1, M: float = 1,mode: str = 'abs') -> np.float32:\n","    vtarget = np.sqrt(G * M / r_target)\n","    r, v = state[:2], state[2:]\n","    dist = np.linalg.norm(r)\n","    rhat = r / dist\n","    rotation_matrix = np.array([[rhat[0], rhat[1]], [-rhat[1], rhat[0]]])\n","    vpolar = rotation_matrix @ v\n","    if mode == 'squared' :\n","      quadratic = w_r*(dist - r_target)**2 + w_p*(vpolar[1]*dist -vtarget*r_target)**2 + w_t*(np.linalg.norm(action))**2\n","    elif mode == 'abs' :\n","      r_dist = 0 if abs(dist - r_target) < 0.05 else abs(dist - r_target)*(1+(iters/128))\n","      mom_dist = 0 if abs(vpolar[1]*dist -vtarget*r_target) < 0.05 else abs(vpolar[1]*dist -vtarget*r_target)*(1+(iters/128))\n","      cum_thrust = 0 if cum_thrust < 10 else cum_thrust\n","      quadratic = w_r*r_dist + w_p*mom_dist + w_c*cum_thrust\n","      #print(f\"{quadratic}\\t\\t{w_r*r_dist}\\t{ w_p*mom_dist}\\t{w_t*abs(np.linalg.norm(action))}\\t{w_c*cum_thrust}\")\n","\n","    return quadratic\n","\n","\n","# SAME HERE (type of reward function ?)\n","def clip_by_norm(t: np.ndarray, mins: float, maxs: float) -> np.ndarray:\n","    norm = np.linalg.norm(t)\n","    if np.count_nonzero(t) == 0 and mins > 0:\n","        raise ValueError('Trying to clip norm of zero vector')\n","    if norm < mins:\n","        t = t * mins / norm\n","    elif norm > maxs:\n","        t = t * maxs / norm\n","\n","    return t\n","\n","def reward_function(iters, state: np.ndarray, action: np.ndarray, rtarget: float, cum_thrust: float,\n","                    w_r: float=1, w_p :float =1,w_t :float =1, w_c: float = 0.0001,\n","                    mode: str = 'Log_10', min :float = 0, max :float = 4, G: float = 1, M: float = 1) -> np.float32:\n","    value = quadratic_penalty(iters, state, action, rtarget, cum_thrust, w_r,w_p,w_t,w_c, G, M)\n","\n","    if mode == 'Quadratic':\n","        return value\n","    elif mode == 'Log_10':\n","        #print(np.clip(-np.log10(value),min,max))\n","        return np.clip(-np.log10(value),min,max)\n","    elif mode == 'Ln' :\n","        return np.clip(-np.log(value),min,max)\n","    elif mode ==  'Inverse' :\n","        return np.clip(1/(value), min, max)\n","    else:\n","        ValueError(f'Invalid reward mode {mode}')\n","\n","class RocketEnv(gym.Env):\n","    def __init__(self,\n","                 G: float = 1, M: float = 1, m: float = .01, dt: float = .01,\n","                 rmin: float = .1, rmax: float = 2, rtarget: float = 1, vmax: float = 10,\n","                 w_r:float = 1, w_p:float =1, w_t: float=1, w_c: float = 0.0001, mode: str = 'Log_10',\n","                 init_func: Callable[[], np.ndarray] = varied_l(),\n","                 oob_penalty: float = 10, max_thrust: float = .1, clip_thrust: str = 'Ball',\n","                 max_step: int = 500, simulation_step: int = 1) -> None:\n","        super().__init__()\n","\n","        #print(\"It is my environment that is running\")\n","        self.observation_space = Box(low=np.array([-rmax, -rmax, -vmax, -vmax]),\n","                                     high=np.array([rmax, rmax, vmax, vmax]),\n","                                     shape=(4, ), dtype=np.float32)\n","        self.action_space = Box(low=np.array([-max_thrust, -max_thrust]),\n","                                high=np.array([max_thrust, max_thrust]),\n","                                shape=(2, ), dtype=np.float32)\n","\n","        self.G, self.M, self.m, self.dt = G, M, m, dt\n","        self.rmin, self.rmax, self.rtarget = rmin, rmax, rtarget\n","        self.vmax = vmax\n","        self.oob_penalty = oob_penalty\n","        self.max_thrust = max_thrust\n","        self.min_action = -self.max_thrust\n","        self.max_action = self.max_thrust\n","        self.clip_thrust = clip_thrust\n","        self.init_func = init_func\n","        self.w_r = w_r\n","        self.w_p = w_p\n","        self.w_t = w_t\n","        self.w_c = w_c\n","        self.mode = mode\n","        self.max_step, self.simulation_step = max_step, simulation_step\n","        self.iters = 0\n","\n","        #things to visualise what's happening\n","        self.reward = 0\n","        self.number_action = 0\n","\n","\n","        # Animation object\n","        lim = rmax * 1.1\n","        self.animation = RocketAnimation(r_min=rmin, r_target=rtarget, r_max=rmax, xlim=(-lim, lim), ylim=(-lim, lim),\n","                                         markersize=10, circle_alpha=1, t_vec_len=.1)\n","        self.cum_thrust = 0\n","\n","    def reset(self, seed: Optional[int] = None,\n","              return_info: bool = False,\n","              options: Optional[dict] = None) -> Union[np.ndarray, Tuple[np.ndarray, dict]]:\n","        super().reset(seed=seed)\n","\n","        if options is not None and 'init_func' in options:\n","            init_func = options['init_f   unc']\n","        else:\n","            init_func = self.init_func\n","\n","        self.state = np.array(init_func())\n","        self.init_state = self.state\n","        self.iters = 0\n","        self.prev_score = - \\\n","            np.abs(self.rmax - self.rtarget) - \\\n","            self.w_p * 2 * self.vmax\n","        self.done = False\n","        self.ep_done = False\n","        self.cum_thrust = 0\n","        self.last_action = np.array([0, 0])\n","        lim = self.rmax * 1.1\n","        if return_info:\n","            return self.state, dict()\n","        else:\n","            return self.state\n","\n","    # CHANGES IN STEP IN ORDERN TO MAKE IT DISCOUNTINOUS :\n","\n","    def step(self, action: np.ndarray) -> Union[Tuple[np.ndarray, float, bool, bool, dict],\n","                                                Tuple[np.ndarray, float, bool, dict]]:\n","        if self.done:\n","            print('Warning: Stepping after done is True')\n","        action = np.array(action)\n","        self.last_action = action\n","        if self.clip_thrust == 'Box':\n","            action = np.clip(action, -1, 1)\n","        elif self.clip_thrust == 'Ball':\n","            magnitude = np.linalg.norm(action)\n","            if magnitude > 1:\n","                action = action / magnitude\n","        elif self.clip_thrust == 'None':\n","            pass\n","        else:\n","            raise ValueError(\n","                f'Thrust clipping mode {self.clip_thrust} does not exist')\n","        r, v = self.state[:2], self.state[2:]\n","        reward = 0\n","        info = dict()\n","        self.cum_thrust += np.abs(action[0])\n","\n","        # SIMULATING STEPS\n","        for _ in range(self.simulation_step):\n","            gravitational_force = - (self.G * self.M * self.m) / \\\n","                (np.power(np.linalg.norm(r), 3)) * r  # F = - GMm/|r|^3 * r\n","            thrust_force = action * self.m * self.max_thrust\n","            total_force = gravitational_force + thrust_force\n","            v = v + total_force / self.m * self.dt\n","            r = r + v * self.dt\n","            self.done = bool(\n","                  np.linalg.norm(r) < self.rmin\n","                  or np.linalg.norm(r) > self.rmax\n","              )\n","            if self.done :\n","              break\n","            reward += reward_function(self.iters, np.array([*r,*v]),action,self.rtarget,self.cum_thrust,\n","                                      w_r = self.w_r, w_p= self.w_p, w_t = self.w_t, w_c = self.w_c,\n","                                      mode = self.mode)\n","        self.reward = reward\n","        self.state = np.array([*r, *v])\n","        self.iters += 1\n","        if self.iters >= self.max_step:\n","            self.ep_done = True\n","            self.done = True\n","        return self.state, self.reward, self.done, self.ep_done, info\n","\n","    def render(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> None:\n","        self.animation.render(self.state, self.last_action, self.last_action,\n","                              self.rmin, self.rtarget, self.rmax)\n","\n","    def show(self, path: Optional[str] = None, summary: bool = False) -> None:\n","        if path is None:\n","            path = \"path.mp4\"\n","        self.animation.save_animation(path)\n","        return path\n","\n","    def get_run_info(self):\n","        return self.animation.get_run_info()\n","\n","class CTangentialThrust(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Provides continous ranges for thrust vectors in the tangential direction\n","    with unit length. Note that it needs to be used in conjuction with PolarizeAction for\n","    the thrust values to point in the tangential direction.\n","    '''\n","\n","    def __init__(self, env: gym.Env):\n","        super().__init__(env)\n","        self.action_space = gym.spaces.Box(low=-self.max_thrust, high= self.max_thrust, shape = (1,), dtype= np.float32)\n","\n","    def action(self, action):\n","        #print(\"It gets there \")\n","        #print(action)\n","        action = action[0].item()\n","        #print(\"here the action is\", action)\n","        self.number_action = action\n","        action = np.array([0,action])\n","        #print(\" the polar action should be :\", action)\n","        state = self.unwrapped.state\n","        #print(\" In order to verify manually : the current state is\", state)\n","        r, v = state[:2], state[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        rotation_matrix = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","        action = rotation_matrix @ action\n","        #print(\" The cartesian coordinate action is \", action)\n","        return action\n","\n","class PolarizeObservation(gym.ObservationWrapper):\n","    def __init__(self, env: gym.Env) -> None:\n","        super().__init__(env)\n","\n","    def observation(self, obs: np.ndarray) -> np.ndarray:\n","        r, v = obs[:2], obs[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        rotation_matrix = np.array([[rhat[0], rhat[1]], [-rhat[1], rhat[0]]])\n","        obs = np.array([dist, *(rotation_matrix @ v)])\n","        return obs"]},{"cell_type":"code","source":[],"metadata":{"id":"3o1MAmRYaq73"},"execution_count":null,"outputs":[]}]}