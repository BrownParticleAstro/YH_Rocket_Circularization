{"cells":[{"cell_type":"markdown","metadata":{"id":"EkOADmeggiJH"},"source":["Aims\n","- Test training model to find r=1 orbit from r=1.1 orbit\n","- Observe std / behavior of model over the course of training\n","\n","Notes:\n","- action = $100$ x model_output\n","- optimal orbit is +/- $0.03$ at $1.1$ (provides OOM of expected error at peak performance)\n","- Kept environment running for $Y$ (used $Y=10,000$) steps after $X$ steps controlled by the model (used $X=50$) and the model is provided bulk reward at the models last controlled step based on the time survived / radius kept to when found from running that model\n","- Experimented with different shapes of punishing deviations from optimal radius (this notebook uses +1/(*error*^3) while [this notebook](https://colab.research.google.com/drive/1asraGMD8RSoxa25xeWv86VRR46vHCU0c#scrollTo=izqzwk2T3g8c) uses the simple -*error* based on the available absolute error range of roughly $[0, \\frac{1}{2}]$)\n","\n","Observations:\n","- basics: model found stable orbits of varying eccentricity\n","- Questions: ideas for how to tune training to get $r$ closer to 1.0?"]},{"cell_type":"markdown","metadata":{"id":"lCHMHxhtharQ"},"source":["### Inits (Imports, Environment, Animation, Model, Functions)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9220,"status":"ok","timestamp":1719850964469,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"},"user_tz":240},"id":"XZZF9O-VhkgL","outputId":"b447d256-c663-4c74-e816-5f357c6e3b62"},"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'rocket_circularization' already exists and is not an empty directory.\n","Requirement already satisfied: matplotlib-label-lines in /usr/local/lib/python3.10/dist-packages (0.7.0)\n","Requirement already satisfied: matplotlib>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from matplotlib-label-lines) (3.7.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from matplotlib-label-lines) (10.1.0)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.10/dist-packages (from matplotlib-label-lines) (1.25.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (4.53.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.2->matplotlib-label-lines) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.2->matplotlib-label-lines) (1.16.0)\n"]}],"source":["## IMPORT BRANCH OF ENVIRONMENT\n","!git clone 'https://github.com/YizhongHu/rocket_circularization.git'\n","!pip install matplotlib-label-lines\n","\n","import os\n","import math\n","import time\n","import glob\n","import random\n","import numpy as np\n","import torch as th\n","import pandas as pd\n","import tensorflow as tf\n","\n","import gym\n","from gym import spaces, logger\n","from gym.utils import seeding\n","from gym.envs.classic_control import utils\n","from gym.error import DependencyNotInstalled\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.distributions import MultivariateNormal\n","from torch.distributions import Categorical\n","\n","from time import sleep\n","from datetime import datetime\n","from typing import Optional, Union\n","from labellines import labelLine, labelLines\n","\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import matplotlib.animation as animation\n","from matplotlib.animation import FuncAnimation\n","from matplotlib.patches import Rectangle\n","from matplotlib.patches import FancyArrowPatch as Arrow\n","\n","from IPython.display import HTML\n","from base64 import b64encode\n","from typing import Any, List, Sequence, Tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rHHkAdegEUE"},"outputs":[],"source":["class PolarizeAction(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Convert polar thrust request to cartesian. Note that the\n","    cartesian thrust value will not rotate with the state during the simulation step.\n","    '''\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap\n","        '''\n","        super().__init__(env)\n","\n","    def action(self, action: np.ndarray) -> np.ndarray:\n","        '''\n","        Convert polar thrust requests to cartesian given the current position.\n","\n","        action: 1D numpy array of shape (2,). The components corresponds to the radial\n","                and tangential components of the thrust respectively.\n","\n","        Return:\n","            numpy array of shape (2,). The thrust in cartesian coordinates.\n","        '''\n","        state = self.unwrapped.state\n","        r, v = state[:2], state[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        rotation_matrix = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","\n","        return rotation_matrix @ action\n","\n","\n","class DiscretiseAction(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Provides 9 discrete thrust levels, no thrust, 4 cardinal directions, and 4 diagonal\n","    directions with unit length. Need to be combined with PolarizeAction to become polar.\n","    '''\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap, preferably with polar thrust\n","        '''\n","        super().__init__(env)\n","\n","        self.action_space = gym.spaces.Discrete(9)\n","        self.thrust_vectors = [\n","            [0, 0]] + [[np.cos(th), np.sin(th)] for th in np.linspace(0, 2 * np.pi, 8, endpoint=False)]\n","\n","    def action(self, action: int) -> np.ndarray:\n","        '''\n","        Map integers to their correspnding thrust value.\n","        0 represents no thrust, 1-8 represents the other directions in counter-clockwise\n","        direction from the x-axis.\n","\n","        action: integer value thrust choice\n","\n","        Return:\n","            1d numpy array of shape (2,), representing the thrust value for this iteration.\n","        '''\n","        action = self.thrust_vectors[action]\n","        return np.ar\n","        ray(action)\n","\n","\n","class TangentialThrust(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Provides 3 discrete thrust vectors in the tangential direction\n","    with unit length. Note that it needs to be used in conjuction with PolarizeAction for\n","    the thrust values to point in the tangential direction.\n","    '''\n","\n","    def __init__(self, env: gym.Env):\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap, preferably with polar thrust\n","        '''\n","\n","        ### MY NOTES WILL HAVE TWO ## from now on because he also put some green comments now\n","        ## ?? the thrust had to be in polar coordinates for the thrust it's way more\n","        ## ?? convenient to have just one axis on which to put the thrust on !\n","\n","\n","        super().__init__(env)\n","        self.action_space = gym.spaces.Discrete(3)\n","\n","\n","        ### ?? Interesting, so the action in this version is decrete (to make it be continous I dont think that it should be too bad, that could be something that I could tryimplementing )\n","        ## For now from my understanding we just have the possibility of going clockwise, have no thrust or counter clockwise\n","\n","        # For more detailed control\n","        # self.thrust_levels = [-1, -0.1, -.01, 0, 0.01, 0.1, 1] (I guess it's for now defined as 1 but we should see if it's too low or too high of an effect on it !)\n","\n","\n","    def action(self, action):\n","\n","        '''\n","        Map integers to their correspnding thrust value.\n","        0: clockwise thrust\n","        1: no thrust\n","        2: counter-clockwise thrust\n","\n","        action: integer value thrust choice\n","\n","        Return:\n","            1d numpy array of shape (2,), representing the thrust value for this iteration.\n","        '''\n","        #print(\"Tangential action running \")\n","        return np.array([0, action - 1])\n","\n","        # For more detailed control\n","        # return np.array([0, self.thrust_levels[action]])\n","\n","\n","class RadialThrust(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Provides 3 discrete thrust vectors in the radial direction\n","    with unit length. Need to be used in conjunction with PolarizeAction for the\n","    thrust values to point in the radial direction.Ã¹\n","    '''\n","\n","    ## ?? My question is with polarize Action is it point in etheta considering a polar coordinate system\n","    ## ?? centered on the mass or is it more to be considered as a Frenet coordinate system where it's just tengential to the motion of the spaceship?\n","    ## or is it a polar centered around the mass M.\n","\n","    ## LOOKING BACK AT THE CODE FOR POLARIZE ACTION, it does indeed consider it with polar coordinates.\n","\n","\n","    def __init__(self, env: gym.Env):\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap, preferably with polar thrust\n","        '''\n","        super().__init__(env)\n","\n","        self.action_space = gym.spaces.Discrete(3)\n","\n","        # For more detailed control\n","        # self.thrust_levels = [-1, -0.3, -0.1, 0, 0.1, 0.3, 1]\n","\n","    def action(self, action: int) -> np.ndarray:\n","        '''\n","        Map integers to their correspnding thrust value.\n","        0: thrust inwards\n","        1: no thrust\n","        2: thrust outwards\n","\n","        action: integer value thrust choice\n","\n","        Return:\n","            1d numpy array of shape (2,), representing the thrust value for this iteration.\n","        '''\n","        # For more detailed control\n","        # return np.array([self.thrust_levels[action], 0])\n","\n","        ## ?? I don't understand at all what that represents the array represents why there is a -1 and why it's two values\n","        ## I actually do, this is in polar coordinates, it has the action that thrust here is declared to be 2 for thrust onward, 1 for no thrust and 0 for thrust inward\n","        ## which should be -1 etheta, 0 ethata and 1.\n","\n","        ### A first suggestion of things to try is just having the action being what it's supposed to be doing, like -1, 0 or 1 not that.\n","\n","        return np.array([action - 1, 0])\n","\n","\n","class PolarizeObservation(gym.ObservationWrapper):\n","    '''\n","    Wrapper for RocketEnv. Wraps state to provide radius, radial velocity, and tangential\n","    velocity.\n","    '''\n","\n","    def __init__(self, env: gym.Env) -> None:\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap\n","        '''\n","        super().__init__(env)\n","\n","    def observation(self, obs: np.ndarray) -> np.ndarray:\n","        '''\n","        Given the observation in cartesian coordinates, convert to polar observation.\n","\n","        obs: numpy array with shape (4,). State vector with first 2 as cartesian position\n","                and last 2 as cartesian velocity\n","\n","        Return\n","            numpy array with shape (3,) with radius, radial velocity, and tangential velocity,\n","            in that order.\n","        '''\n","        r, v = obs[:2], obs[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        rotation_matrix = np.array([[rhat[0], rhat[1]], [-rhat[1], rhat[0]]])\n","        obs = np.array([dist, *(rotation_matrix @ v)])\n","        return obs\n","\n","\n","class RadialObservation(gym.ObservationWrapper):\n","    def __init__(self, env: gym.Env) -> None:\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap\n","        '''\n","        super().__init__(env)\n","\n","    def observation(self, obs: np.ndarray) -> np.ndarray:\n","        '''\n","        Given the observation in cartesian coordinates, convert to observation with\n","        only radial position and velocity.\n","\n","        obs: numpy array with shape (4,). State vector with first 2 as cartesian position\n","                and last 2 as cartesian velocity\n","\n","        Return\n","            numpy array with shape (2,) with radius and radial velocity, in that order.\n","        '''\n","        r, v = obs[:2], obs[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        obs = np.array([dist, v @ rhat])\n","        return obs\n","\n","class CTangentialThrust(gym.ActionWrapper):\n","    '''\n","    Wrapper for RocketEnv. Provides continous ranges for thrust vectors in the tangential direction\n","    with unit length. Note that it needs to be used in conjuction with PolarizeAction for\n","    the thrust values to point in the tangential direction.\n","    '''\n","\n","    def __init__(self, env: gym.Env):\n","        '''\n","        Initialize the wrapper.\n","\n","        env: The environment to wrap, preferably with polar thrust\n","        '''\n","        super().__init__(env)\n","        self.action_space = gym.spaces.Box(low=-self.max_thrust, high= self.max_thrust, shape = (1,), dtype= np.float32)\n","\n","    def action(self, action):\n","        '''\n","        Map integers to their correspnding thrust value.\n","        0: clockwise thrust\n","        1: no thrust\n","        2: counter-clockwise thrust\n","\n","        action: integer value thrust choice\n","\n","        Return:\n","            1d numpy array of shape (2,), representing the thrust value for this iteration.\n","        '''\n","        #print(\"It gets there \")\n","        action = np.array([0,action])\n","        #print(\" the polar action should be :\", action)\n","        state = self.unwrapped.state\n","        #print(\" In order to verify manually : the current state is\", state)\n","        r, v = state[:2], state[2:]\n","        dist = np.linalg.norm(r)\n","        rhat = r / dist\n","        rotation_matrix = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","        action = rotation_matrix @ action\n","        #print(\" The cartesian coordinate action is \", action)\n","        return action\n","\n","from logging import raiseExceptions\n","from typing import (\n","    Any,\n","    Generic,\n","    Iterable,\n","    List,\n","    Mapping,\n","    Optional,\n","    Sequence,\n","    Tuple,\n","    Type,\n","    TypeVar,\n","    Union,\n","    Dict,\n","    Callable\n",")\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gym\n","from gym.spaces import Box\n","from gym.utils import seeding\n","\n","#from animation import RocketAnimation\n","\n","\n","def make(name):\n","    '''\n","    Initialize Rocket Circularization environment. Contains the environment hyperparameters\n","\n","    `RocketCircularization-v0` has wall mechanics on. When the craft hits the boundary, it will\n","    loose all orthogonal velocity towards the boundary.\n","    `RocketCircularization-v1` has wall mechanics off. When the craft hits the boundary, it will\n","    pass through boundary and mark the state as truncated.\n","\n","    Usage:\n","    ```python\n","    env = make('RocketCircularization-v0')\n","    ```\n","    or in notebooks\n","    ```python\n","    with make('RocketCircularization-v0') as env:\n","        ...\n","    ```\n","    '''\n","    if name == 'RocketCircularization-v0':\n","        init_func = varied_l(r_min=0.5, r_max=1.5)\n","        return RocketEnv(max_step=400, simulation_step=1, rmax=1.5, rmin=0.5,\n","                         init_func=init_func, max_thrust=10,\n","                         oob_penalty=0, dt=0.01, wall_mechanics=False, clip_thrust='None',\n","                         velocity_penalty_rate=0.1, thrust_penalty_rate=0.001)\n","    if name == 'RocketCircularization-v1':\n","        return RocketEnv(max_step=1000, simulation_step=1, rmax=1.1, rmin=0.9, max_thrust=10,\n","                         oob_penalty=0, dt=0.01, wall_mechanics=False, clip_thrust='None',\n","                         velocity_penalty_rate=0.1, thrust_penalty_rate=0.001)\n","    else:\n","        raise ValueError(f'No environment {name}')\n","\n","\n","def uniform(r_min: float = 0.99, r_max: float = 1.01,\n","            rdot_min: float = -0.05, rdot_max: float = 0.05,\n","            thetadot_min: float = 0.99, thetadot_max: float = 1.01) \\\n","        -> Callable[[], List[np.float32]]:\n","    '''\n","    Produces a function that generates initial conditions at different angles uniformly under those\n","    conditions\n","\n","    r_min: minimum bound for radius\n","    r_max: maximum bound for radius\n","    rdot_min: minimum bound for radial velocity\n","    rdot_max: maximum bound for radial velocity\n","    thetadot_min: minimum bound for angular velocity\n","    thetadot_max: maximum bound for angular velocity\n","\n","    Return:\n","        A function that when called, returns an initial condition\n","    '''\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max, thetadot_min, thetadot_max\n","\n","        r = np.random.uniform(r_min, r_max)\n","        theta = np.random.uniform(0, 2 * np.pi)\n","        rdot = np.random.uniform(rdot_min, rdot_max)\n","        thetadot = np.random.uniform(thetadot_min, thetadot_max)\n","\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","\n","def varied_l(r_min: float = 0.9, r_max: float = 1.1,\n","             rdot_min: float = -0.5, rdot_max: float = 0.5,\n","             dl_min: float = -.5, dl_max: float = .5) \\\n","        -> Callable[[], List[np.float32]]:\n","    '''\n","    Produces a function that generates initial conditions at different angles uniformly with\n","    a range of angular momentum settings\n","\n","    r_min: minimum bound for radius\n","    r_max: maximum bound for radius\n","    rdot_min: minimum bound for radial velocity\n","    rdot_max: maximum bound for radial velocity\n","    dl_min: minimum deviation of angular momentum from target angular momentum\n","    dl_max: maximum deviation of angular momentum from target angular momentum\n","\n","    Return:\n","        A function that when called, returns an initial condition\n","    '''\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max\n","\n","        # r = np.random.uniform(r_min, r_max)\n","        # theta = np.random.uniform(0, 2 * np.pi)\n","        # rdot = np.random.uniform(rdot_min, rdot_max)\n","        # thetadot = (1 + np.random.uniform(dl_min, dl_max)) / r ** 2\n","\n","        r = (r_min + r_max) / 2\n","        theta = 0\n","        rdot = 0\n","        thetadot = 0\n","\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","\n","def target_l(r_min: float = 0.5, r_max: float = 1.5,\n","             rdot_min: float = -0.5, rdot_max: float = 0.5) \\\n","        -> Callable[[], List[np.float32]]:\n","    '''\n","    Produces a function that generates initial conditions at different angles uniformly\n","    with the target angular momentum\n","\n","    r_min: minimum bound for radius\n","    r_max: maximum bound for radius\n","    rdot_min: minimum bound for radial velocity\n","    rdot_max: maximum bound for radial velocity\n","\n","    Return:\n","        A function that when called, returns an initial condition\n","    '''\n","    def func():\n","        nonlocal r_min, r_max, rdot_min, rdot_max\n","\n","        r = np.random.uniform(r_min, r_max)\n","        theta = np.random.uniform(0, 2 * np.pi)\n","        rdot = np.random.uniform(rdot_min, rdot_max)\n","        thetadot = 1 / r ** 2\n","\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","def testing_1() -> Callable[[], List[np.float32]]:\n","  def func():\n","    r = 1\n","    theta = np.random.uniform(0, 2 * np.pi)\n","    rdot = 0\n","    thetadot = 1\n","\n","    pos = [r, 0]\n","    vel = [rdot, r * thetadot]\n","\n","    rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","\n","    return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","  return func\n","\n","\n","\n","\n","def quadratic_penalty(state: np.ndarray, action: np.ndarray, rtarget: float,\n","                      velocity_penalty_rate: float, thrust_penalty_rate: float,\n","                      G: float = 1, M: float = 1) -> np.float32:\n","    '''\n","    Calculates the Quadratic reward at the current state with the current action. Subject to change.\n","\n","    reward = -(r - rtarget)^2 - velocity_penalty * (v_r^2 + (v_t - v_t,target)^2) - thrust_penalty * |u|^2\n","\n","    state: the current game state\n","    action: actions performed to reach this state\n","    rtarget: target radius of the craft\n","    velocity_penalty_rate: ratio of velocity penalty to radius penalty\n","    thrust_penalty_rate: ratio of thrust penalty to radius penalty\n","\n","    G: Gravitational Constant, default 1\n","    M: Mass of center object, default 1\n","\n","    Return:\n","        Reward in this state\n","    '''\n","    vtarget = np.sqrt(G * M / rtarget)\n","    r, v = state[:2], state[2:]\n","    dist = np.linalg.norm(r)\n","    rhat = r / dist\n","    rotation_matrix = np.array([[rhat[0], rhat[1]], [-rhat[1], rhat[0]]])\n","    vpolar = rotation_matrix @ v\n","\n","    return -((dist - rtarget)**2) \\\n","        - velocity_penalty_rate * (vpolar[0] ** 2 + (vpolar[1] - vtarget)**2) \\\n","        - thrust_penalty_rate * np.linalg.norm(action) ** 2\n","\n","\n","def reward_function(state: np.ndarray, action: np.ndarray, rtarget: float,\n","                    velocity_penalty_rate: float, thrust_penalty_rate: float,\n","                    mode: str = 'Quadratic', G: float = 1, M: float = 1) -> np.float32:\n","    '''\n","    Calculates the reward at the current state with the current action. Subject to change.\n","\n","    state: the current game state\n","    action: actions performed to reach this state\n","    rtarget: target radius of the craft\n","    velocity_penalty_rate: ratio of velocity penalty to radius penalty\n","    thrust_penalty_rate: ratio of thrust penalty to radius penalty\n","    mode: Mode of rewards, one of 'Quadratic' or 'Gaussian'\n","            Quadratic: -(r - rtarget)^2 - velocity_penalty * (v_r^2 + (v_t - v_t,target)^2) - thrust_penalty * |u|^2\n","            Gaussian: exp(-(r - rtarget)^2 - velocity_penalty * (v_r^2 + (v_t - v_t,target)^2) - thrust_penalty * |u|^2)\n","\n","    G: Gravitational Constant, default 1\n","    M: Mass of center object, default 1\n","\n","    Return:\n","        Reward in this state\n","    '''\n","    value = quadratic_penalty(state, action, rtarget,\n","                              velocity_penalty_rate, thrust_penalty_rate, G, M)\n","\n","    if mode == 'Quadratic':\n","        return value\n","    elif mode == 'Gaussian':\n","        return np.exp(value)\n","    else:\n","        ValueError(f'Invalid reward mode {mode}')\n","\n","\n","def score(state: np.ndarray, rtarget: float,  velocity_penalty_rate: float,\n","          G: float = 1, M: float = 1) -> np.float32:\n","    '''\n","    DEPRECATED\n","    Calculates the reward at the current state without action penalty. Subject to change.\n","    This may be used for a differential reward structure\n","\n","    score = -(r - rtarget)^2 - velocity_penalty * (v_r^2 + (v_t - v_t,target)^2)\n","\n","    state: the current game state\n","    rtarget: target radius of the craft\n","    velocity_penalty_rate: ratio of velocity penalty to radius penalty\n","\n","    G: Gravitational Constant, default 1\n","    M: Mass of center object, default 1\n","\n","    Return:\n","        Score in this state\n","    '''\n","    vtarget = np.sqrt(G * M / rtarget)\n","    r, v = state[:2], state[2:]\n","    dist = np.linalg.norm(r)\n","    rhat = r / dist\n","    rotation_matrix = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","    vtarget = rotation_matrix @ np.array([0, vtarget])\n","\n","    return -np.abs(dist - rtarget) - velocity_penalty_rate * np.sum(np.abs(v - vtarget))\n","\n","\n","# def reward_function(state: np.ndarray, action: np.ndarray, prev_score: np.float32, rtarget: float,\n","#                     velocity_penalty_rate: float, thrust_penalty_rate: float,\n","#                     G: float=1, M: float=1) -> np.float32:\n","#     '''\n","#     DEPRECATED\n","#     Calculates the reward at the current state with action penalty. Subject to change.\n","#     This may be used for a differential reward structure\n","\n","#     reward = current_score - prev_score + thrust_penalty * |u|^2\n","\n","#     state: the current game state\n","#     action: actions performed to reach this state\n","#     prev_score: score from the last time the reward is calculated\n","#     rtarget: target radius of the craft\n","#     velocity_penalty_rate: ratio of velocity penalty to radius penalty\n","#     thrust_penalty_rate: ratio of thrust penalty to radius penalty\n","\n","#     G: Gravitational Constant, default 1\n","#     M: Mass of center object, default 1\n","\n","#     Return:\n","#         Reward at this state\n","#     '''\n","#     curr_score = score(state, rtarget, velocity_penalty_rate, G=G, M=M)\n","#     return curr_score - prev_score - thrust_penalty_rate * np.sum(np.abs(action)), curr_score\n","\n","\n","def clip_by_norm(t: np.ndarray, mins: float, maxs: float) -> np.ndarray:\n","    '''\n","    Clip the vector by its l2 norm between an interval.\n","\n","    t: the vector to clip\n","    mins: the minimum norm\n","    maxs: the maximum norm\n","\n","    Return:\n","        Clipped vector\n","\n","    Raises:\n","        ValueError: when norm of input vector is zero and minimum is not zero\n","    '''\n","    norm = np.linalg.norm(t)\n","    if np.count_nonzero(t) == 0 and mins > 0:\n","        raise ValueError('Trying to clip norm of zero vector')\n","    if norm < mins:\n","        t = t * mins / norm\n","    elif norm > maxs:\n","        t = t * maxs / norm\n","\n","    return t\n","\n","\n","def wall_clip_velocity(v: np.ndarray, r: np.ndarray, mins: float, maxs: float):\n","    '''\n","    If the particle is moving towards the circular boundaries, cancel velocity perpendicular to the boundary\n","\n","    v: velocity vector\n","    r: position vector\n","    mins: minimum radius\n","    maxs: maximum radius\n","\n","    Return:\n","        Velocity vector modified by the walls\n","    '''\n","    # Obtain the velocity component orthogonal to the circular boundaries\n","    direction = v @ r\n","    along = (v @ r) / (r @ r) * r\n","    ortho = v - along\n","\n","    # Get the distance from origin to test if the object is at the bounds\n","    dist = np.linalg.norm(r)\n","\n","    # If there is a component facing in at minimum radius\n","    if dist < mins and direction < 0:\n","        return ortho\n","    # If there is a component facing out at maximum radius\n","    elif dist > maxs and direction > 0:\n","        return ortho\n","    else:\n","        return v\n","\n","\n","class RocketEnv(gym.Env):\n","    '''\n","    Open AI Gym environment for Rocket Circularization\n","    '''\n","\n","    def __init__(self,\n","                 G: float = (2*np.pi)**2, M: float = 1, m: float = .01, dt: float = .01,\n","                 rmin: float = .1, rmax: float = 2, rtarget: float = 1, vmax: float = 10,\n","                 init_func: Callable[[], np.ndarray] = varied_l(), wall_mechanics: bool = True,\n","                 oob_penalty: float = 10, max_thrust: float = 10, clip_thrust: str = 'Ball',\n","                 velocity_penalty_rate: float = .001, thrust_penalty_rate: float = .0001,\n","                 max_step: int = 500, simulation_step: int = 1) -> None:\n","        '''\n","        Initializes the environment\n","\n","        G: Gravitational Constant, default 1\n","        M: Mass of center object, default 1\n","        m: Mass of orbiting object, default .01\n","        dt: Simulation time step, default .01\n","\n","        rmin: game space radius lower bound, default .1\n","        rmax: game space radius upper bound, default 2\n","        rtarget: the target radius the craft is supposed to reach, default 1\n","        vmax: maximum velocity allowed in the game space (implemented for simulation accuracy and network interpolation), default 10\n","        oob_penalty: penalty for being out of bounds, default 10\n","        init_func: function that returns an initial condition, default varied_l()\n","        wall_mechanics: whether the boundary acts as a wall.\n","                If true, all normal velocity towards the boundary will be canceled upon impact\n","                If false, the craft will pass through the wall and truncation will be marked true\n","\n","        max_thrust: The magnitude of the thrust, scales the action u\n","        clip_thrust: The way in which the action is clipped, Options: Box, Ball, None, default: Ball\n","\n","        velocity_penalty_rate: the penalty of velocity as a ratio of radius penalty\n","        thrust_penalty_rate: the penalty of thrust as a ratio of radius penalty\n","\n","        max_step: number of iterations in each episode if no early-termination is encountered, default: 500\n","        simulation_step: number of simulation steps for every game step. Reducing timestep and increasing simulation step\n","                increases simulation accuracy, but may be more computationally straining\n","        '''\n","        super().__init__()\n","\n","        #print(\"It is my environment that is running\")\n","        self.observation_space = Box(low=np.array([-rmax, -rmax, -vmax, -vmax]),\n","                                     high=np.array([rmax, rmax, vmax, vmax]),\n","                                     shape=(4, ), dtype=np.float32)\n","        self.action_space = Box(low=np.array([-max_thrust, -max_thrust]),\n","                                high=np.array([max_thrust, max_thrust]),\n","                                shape=(2, ), dtype=np.float32)\n","\n","        self.G, self.M, self.m, self.dt = G, M, m, dt\n","        self.rmin, self.rmax, self.rtarget = rmin, rmax, rtarget\n","        self.vmax = vmax\n","        self.oob_penalty = oob_penalty\n","        self.max_thrust = max_thrust\n","        self.clip_thrust = clip_thrust\n","        self.init_func = init_func\n","        self.wall_mechanics = wall_mechanics\n","        self.velocity_penalty_rate = velocity_penalty_rate\n","        self.thrust_penalty_rate = thrust_penalty_rate\n","\n","        self.max_step, self.simulation_step = max_step, simulation_step\n","        self.iters = 0\n","\n","        # Animation object\n","        lim = rmax * 1.1\n","        self.animation = RocketAnimation(r_min=rmin, r_target=rtarget, r_max=rmax, xlim=(-lim, lim), ylim=(-lim, lim),\n","                                         markersize=10, circle_alpha=1, t_vec_len=.1)\n","\n","    def reset(self, seed: Optional[int] = None,\n","              return_info: bool = False,\n","              options: Optional[dict] = None) -> Union[np.ndarray, Tuple[np.ndarray, dict]]:\n","        '''\n","        Resets the environment with a new state, return the new state as well as information\n","        if required.\n","\n","        seed: NOT IMPLEMENTED, randomizer seed\n","        return_info: If information is returned\n","        options: some options for initialization\n","                init_func: the function used to initialize the state, provided uniform, target_l, varied_l\n","\n","        Return:\n","            the initial state, shape (4,)\n","        '''\n","        # super().reset(seed=seed)\n","\n","        if options is not None and 'init_func' in options:\n","            init_func = options['init_func']\n","        else:\n","            init_func = self.init_func\n","\n","        self.state = np.array(init_func())\n","        self.init_state = self.state\n","        self.iters = 0\n","        self.prev_score = - \\\n","            np.abs(self.rmax - self.rtarget) - \\\n","            self.velocity_penalty_rate * 2 * self.vmax\n","        self.done = False\n","        self.last_action = np.array([0, 0])\n","\n","        lim = self.rmax * 1.1\n","        self.animation = RocketAnimation(r_min=self.rmin, r_target=self.rtarget, r_max=self.rmax,\n","                                         xlim=(-lim, lim), ylim=(-lim, lim),\n","                                         markersize=10, circle_alpha=1, t_vec_len=.1)\n","\n","        if return_info:\n","            return self.state, dict()\n","        else:\n","            return self.state\n","\n","    def step(self, action: np.ndarray) -> Union[Tuple[np.ndarray, float, bool, bool, dict],\n","                                                Tuple[np.ndarray, float, bool, dict]]:\n","        '''\n","        Accept action and modify the states accordingly.\n","\n","        action: an array with shape (2,) representing the thrust component in the 2 cartesian directions.\n","\n","        Return:\n","            state, shape (2,),\n","            reward from this state,\n","            if the game is done running,\n","            if the agent is out-of-bounds, and\n","            some more info about the game\n","        '''\n","        if self.done:\n","            print('Warning: Stepping after done is True')\n","\n","        # Clipp action if needed\n","        action = np.array(action)\n","        if self.clip_thrust == 'Box':\n","            action = np.clip(action, -1, 1)\n","        elif self.clip_thrust == 'Ball':\n","            magnitude = np.linalg.norm(action)\n","            if magnitude > 1:\n","                action = action / magnitude\n","        elif self.clip_thrust == 'None':\n","            pass\n","        else:\n","            raise ValueError(\n","                f'Thrust clipping mode {self.clip_thrust} does not exist')\n","\n","        # For easy access from wrappers\n","        self.last_action = action\n","\n","        r, v = self.state[:2], self.state[2:]\n","        reward = 0\n","        info = dict()\n","\n","        # Simulate for a number of steps\n","        for _ in range(self.simulation_step):\n","            # Calculate total force\n","            gravitational_force = - (self.G * self.M * self.m) / \\\n","                (np.power(np.linalg.norm(r), 3)) * r  # F = - GMm/|r|^3 * r\n","            thrust_force = action * self.m * self.max_thrust\n","            total_force = gravitational_force + thrust_force\n","            # Update position and location, this can somehow guarantee energy conservation\n","            # If the craft hits a wall, all normal velocity cancels\n","            v = v + total_force / self.m * self.dt\n","            # v = clip_by_norm(v, 0, self.vmax)\n","            r = r + v * self.dt\n","            if self.wall_mechanics:\n","                v = wall_clip_velocity(v, r, self.rmin, self.rmax)\n","                r = clip_by_norm(r, self.rmin, self.rmax)\n","            # Scored-based reward structure\n","            reward += reward_function(np.array([*r, *v]), action,\n","                                      self.rtarget, self.velocity_penalty_rate,\n","                                      self.thrust_penalty_rate, 'Quadratic', self.G, self.M)\n","            # Differential-score-based Reward structure\n","            # step_reward, self.prev_score = reward_function(np.array([*r, *v]), action, self.prev_score, self.rtarget, self.velocity_penalty_rate,\n","            #                                                self.thrust_penalty_rate, self.G, self.M)\n","            # reward += step_reward * self.dt\n","\n","            # If out-of-bounds, end the game\n","            if self.wall_mechanics:\n","                # The game will not be truncated when wall-mechanics are disabled\n","                truncated = False\n","            else:\n","                # This condition may be changed into a controllability condition\n","                if np.linalg.norm(r) > self.rmax or np.linalg.norm(r) < self.rmin:\n","                    reward -= self.oob_penalty\n","                    truncated = True\n","                    # self.state = self.init_state\n","                else:\n","                    self.state = np.array([*r, *v])\n","                    truncated = False\n","\n","        self.state = np.array([*r, *v])\n","        self.iters += 1\n","\n","        if self.iters >= self.max_step:\n","            self.done = True\n","\n","        return self.state, reward, self.done, truncated, info\n","\n","    def render(self, *args: Tuple[Any], **kwargs: Dict[str, Any]) -> None:\n","        '''\n","        Record frames of the animation. Need to be used in conjunction with the\n","        show() method.\n","        '''\n","        self.animation.render(self.state, self.last_action, self.last_action,\n","                              self.rmin, self.rtarget, self.rmax)\n","\n","    def show(self, path: Optional[str] = None, summary: bool = False) -> None:\n","        '''\n","        Show the saved frames of the animation or produce a summary\n","\n","        path: if the animation is saved, the path to which it is saved. If None, the\n","                animation is shown in a pop-up window. Note that pop-up window\n","                animation does not work in notebooks, and need to be closed for\n","                the execution to continue. default, None\n","        summary: if animation is not saved, if to produce a summary plot of the game\n","                episode instead.\n","        '''\n","        if path is None:\n","            path = \"path.mp4\"\n","        self.animation.save_animation(path)\n","        self.animation.summary_plot()\n","        return path\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1719850964470,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"},"user_tz":240},"id":"C7htV17rhEG5","outputId":"94c9ef41-f7d8-42c7-a44e-04dc3c7e5582"},"outputs":[{"output_type":"stream","name":"stderr","text":["<>:144: DeprecationWarning: invalid escape sequence '\\h'\n","<ipython-input-30-fbe696025f92>:2: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n","  plt.style.use('seaborn-pastel')\n"]}],"source":["# matplotlib.use('TkAgg')\n","plt.style.use('seaborn-pastel')\n","\n","class RocketAnimation(object):\n","    def __init__(self, r_min=0.1, r_target=1, r_max=10, xlim=(-10.2, 10.2), ylim=(-10.2, 10.2), markersize=10, circle_alpha=1, t_vec_len=1):\n","        '''\n","        Initialize Animation Object\n","\n","        Parameters:\n","            r_min: the minimum radius circle\n","            r_target: the target radius circle\n","            r_max: the maximum radius circle\n","            x_lim: tuple of 2 elements, max and min bound of the axes on the x direction\n","            y_lim: tuple of 2 elements, max and min bound of the axes on the y direction\n","            markersize: int, the size of the marker indicating rocket\n","            t_vec_len: the scale of the thrust vector\n","        '''\n","\n","\n","        self.r_min = r_min\n","        self.r_target = r_target\n","        self.r_max = r_max\n","\n","        self.marker_size = markersize\n","        self.circle_alpha = circle_alpha\n","        self.t_vec_len = t_vec_len\n","\n","        self.states = list()\n","        self.thrusts = list()\n","        # Note : I am not using requested_thrust for now, but I let it because I think it could be interesting\n","        self.requested_thrusts = list()\n","        self.theta_dot = list()\n","\n","        # Wait why on earth is that a list ??\n","        self.rmin = list()\n","        self.rtarget = list()\n","        self.rmax = list()\n","\n","        self.xlim = xlim\n","        self.ylim = ylim\n","\n","    #No changes are made to this function\n","    def _circle(self, radius):\n","        '''\n","        Create data for a circle with a certain radius\n","\n","        Parameters:\n","            radius: the radius of the circle\n","\n","        Return:\n","            tuple of np.ndarray representing the coordinates of each point\n","            on the circle\n","        '''\n","        theta = np.linspace(0, 2 * np.pi, 100)\n","        x, y = radius * np.cos(theta), radius * np.sin(theta)\n","        return x, y\n","\n","    def _init(self,):\n","        '''\n","        Function used for generating the animation\n","        The first step in the animation\n","\n","        Returns:\n","            line to update\n","        '''\n","        #Only initiating the main line to update\n","\n","        # this is necesssary because for some reasons it is called 3 times\n","        self.ax.clear()\n","\n","        self.arrow = Arrow(posA=(0, 0), posB=(0, 0), arrowstyle='simple', mutation_scale=5, color='r')\n","        self.ax.add_patch(self.arrow)\n","        self.line, = self.ax.plot([], [], marker='o', markersize=self.marker_size, alpha=self.circle_alpha)\n","        self.min_circle, = self.ax.plot(*self._circle(self.r_min), '--', label='Minimum Radius', color='tab:blue')\n","        self.start_circle, = self.ax.plot(*self._circle(1.1), '--', label='Starting Orbit', color='tab:orange') ##### TARGET = 1.1\n","        self.target_circle, = self.ax.plot(*self._circle(self.r_target), '--', label='Target Orbit', color='tab:green')\n","        self.max_circle, = self.ax.plot(*self._circle(self.r_max), '--', label='Maximum Radius', color='tab:purple')\n","        self.ax.grid(True)\n","        self.ax.legend(loc='upper left')\n","        return self.line, self.min_circle, self.target_circle, self.max_circle\n","\n","\n","    def _animate(self, i):\n","        '''\n","        Function used for generating the animation\n","        The update function run each time the animation advances\n","\n","        Parameters:\n","            i: the number of frames of the animation\n","\n","        Returns:\n","            line to update\n","        '''\n","        st = self.states[i]\n","        vec = self.thrusts[i] * self.t_vec_len * (self.xlim[1] - self.xlim[0])\n","\n","        self.line.set_data([st[0]], [st[1]])\n","        self.min_circle.set_data(*self._circle(self.rmin[i]))\n","        self.target_circle.set_data(*self._circle(self.rtarget[i]))\n","        self.max_circle.set_data(*self._circle(self.rmax[i]))\n","\n","        self.arrow.set_positions(posA=st[:2], posB=st[:2] + vec)\n","        self.fig.suptitle(f'Iteration: {i}')\n","        return self.line, self.min_circle, self.target_circle, self.max_circle\n","\n","    ## DEFINING A NEW ANIMATION FUNCTION TO ONLY HAVE SUMMARY AND THE LITTLE VIDEO\n","\n","    def save_animation(self, name):\n","        '''\n","        Save the animation in a file\n","\n","        Parameter:\n","            name: str, the file name\n","        '''\n","        #print('save animation is called')\n","\n","        self._transform_vectors()\n","        self.fig = plt.figure(figsize=(10, 10), num=1,\n","                              clear=True, tight_layout=True)\n","        #print(\"figure instantiated \")\n","        self.ax = self.fig.add_subplot(111)\n","        anim = FuncAnimation(self.fig, self._animate, init_func=self._init, frames=len(\n","            self.states), blit=True, interval=100, repeat=False)\n","        anim.save(name, dpi=80)\n","\n","    def _plot_thrust_magnitude(self, ax):\n","        #print(\"plot thrust magnitude is called \")\n","        ax.set_title('Thrust Magnitude')\n","        ax.plot(self.thrusts_norm, label='thrust magnitude')\n","        ax.plot(self.requested_thrusts_norm,\n","                    label='requested thrust magnitude')\n","        ax.grid(True)\n","        ax.legend()\n","\n","## CHANGED\n","    def _plot_thrust_value(self, ax):\n","        ax.set_title('Thrust Values')\n","        ax.plot([thrust[1]\n","                     for thrust in self.thrusts_polar], label='thrust tangent')\n","        ax.grid(True)\n","        ax.legend()\n","\n","    def _plot_thrust_direction(self, ax):\n","        ax.set_title('Thrust Direction (Angle from $\\hat{r}$)')\n","        ax.plot(self.thrust_direction, label='Thrust Direction')\n","        ax.plot(self.requested_thrust_direction,\n","                label='Requested Thrust Direction')\n","        ax.grid(True)\n","        ax.legend()\n","\n","    def _plot_radius(self, ax):\n","        ax.set_title('Radius')\n","        ax.plot(self.rs, label='radius')\n","        ax.grid(True)\n","        ax.legend()\n","\n","    def _plot_velocities(self, ax):\n","        ax.set_title('Velocities')\n","        ax.plot([vel[0] for vel in self.vel_polar], label='radial velocity')\n","        ax.plot([vel[1] for vel in self.vel_polar],\n","                label='tangential velocity')\n","        ax.grid(True)\n","        ax.legend()\n","\n","## ADDED\n","\n","    def _plot_theta_dot(self, ax):\n","      ax.set_title('theta_dot')\n","      ax.plot([theta for theta in self.theta_dot])\n","      ax.grid(True)\n","\n","    def summary_plot(self):\n","        self._transform_vectors()\n","        print(\"It is creating a figure 1 \")\n","        self.fig1 = plt.figure(figsize=(10, 5), num=1,\n","                              clear=True, tight_layout=True)\n","        ax1 = self.fig1.add_subplot(221)\n","        ax2 = self.fig1.add_subplot(222)\n","        ax3 = self.fig1.add_subplot(223)\n","        ax4 = self.fig1.add_subplot(224)\n","\n","        self.fig1.suptitle('Run Summary')\n","\n","\n","        self._plot_thrust_value(ax1)\n","        self._plot_theta_dot(ax2)\n","        self._plot_radius(ax3)\n","        self._plot_velocities(ax4)\n","\n","        self.fig1.tight_layout()\n","\n","        return self.fig1\n","\n","    def show(self, name):\n","      # saving the animation\n","      self.save_animation(name)\n","      #creating a summary\n","      self.summary_plot()\n","\n","\n","\n","\n","\n","    def _get_transforms(self, states):\n","        # So basically transforms is the list of all the rotation matrices\n","        transforms = list()\n","        rs = list()\n","        thetas = list()\n","\n","        for st in states:\n","            # position and velocity in cartesian coordinates\n","            pos, vel = st[:2], st[2:]\n","            # norm of the position (s)\n","            r = np.linalg.norm(pos)\n","            #getting theta\n","            theta = np.arctan2(pos[1], pos[0])\n","            # rhat in cartesian\n","            rhat = pos / r\n","\n","            rot_mat = np.array([[rhat[0], -rhat[1]], [rhat[1], rhat[0]]])\n","            transforms.append(rot_mat)\n","            # all the radius\n","            rs.append(r)\n","            #all the angle it has been at\n","            thetas.append(theta)\n","\n","        return transforms, rs, thetas\n","\n","    def _forward_transform(self, transforms, vecs):\n","        return [tr @ vec for tr, vec in zip(transforms, vecs)]\n","\n","    # it can get the vector in its original position technically if vec = T@vec\n","    def _inverse_transform(self, transforms, vecs):\n","        return [tr.T @ vec for tr, vec in zip(transforms, vecs)]\n","\n","\n","    def _transform_vectors(self, ):\n","        #tranforms : rot matrix, rs : the radius, thetas : the angles\n","        transforms, self.rs, self.thetas = self._get_transforms(self.states)\n","\n","        #gets the polar velocities\n","        self.vel_polar = self._inverse_transform(\n","            transforms, [st[2:] for st in self.states])\n","\n","        # it's the last action in cartesian coordinate\n","        self.thrusts_polar = self._inverse_transform(transforms, self.thrusts)\n","\n","        #This for now is still defined as the last action in cartesian coordinates (but could be interesting to implement)\n","        self.requested_thrusts_polar = self._inverse_transform(\n","            transforms, self.requested_thrusts)\n","\n","        # ADDING THIS\n","        self.theta_dot = [v_tangential / r if r != 0 else 0 for v_tangential, r in zip([vel[1] for vel in self.vel_polar], self.rs)]\n","\n","        ## CHANGE HERE : WE HAVE THE THRUST IN POLAR, NO NEED FOR THE NORM (THOSE FOUR UNECESSARY)\n","        self.thrusts_norm = [np.linalg.norm(thrust) for thrust in self.thrusts]\n","        self.requested_thrusts_norm = [np.linalg.norm(\n","            thrust) for thrust in self.requested_thrusts]\n","        self.thrust_direction = [np.arctan2(\n","            thrust[1], thrust[0]) for thrust in self.thrusts_polar]\n","        self.requested_thrust_direction = [np.arctan2(\n","            thrust[1], thrust[0]) for thrust in self.requested_thrusts_polar]\n","\n","      ## MAYBE ADD THETA DOT\n","\n","    def render(self, state, thrust, requested_thrust, rmin, rtarget, rmax):\n","        '''\n","        Records the current state in the animation for future rendering\n","\n","        Parameters:\n","            state: the current state to render\n","        '''\n","        # I DON'T UNDERDSTAND THE POINT OF HAVING AN R_MIN AND R_MAX DEFINED LIKE THAT !!\n","        self.states.append(state)\n","        self.thrusts.append(thrust)\n","        self.requested_thrusts.append(requested_thrust)\n","        self.rmin.append(rmin)\n","        self.rtarget.append(rtarget)\n","        self.rmax.append(rmax)\n","\n","\n","if __name__ == '__main__':\n","    pass"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1719850964470,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"},"user_tz":240},"id":"poG4dZohhgqT","outputId":"2c34798e-c964-495d-bef6-51cbff9a4239"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}],"source":["def show_video(video_path, video_width = 600):\n","  video_file = open(video_path, \"r+b\").read()\n","  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n","  return HTML(f\"\"\"<video width={video_width} controls><source src=\"{video_url}\"></video>\"\"\")\n","\n","\n","def moving_average(data, window_size):\n","    \"\"\"Calculate the moving average of the given list.\"\"\"\n","    i = 0\n","    moving_averages = []\n","    while i < len(data) - window_size + 1:\n","        window_average = round(np.sum(data[\n","          i:i+window_size]) / window_size, 2)\n","        moving_averages.append(window_average)\n","        i += 1\n","    return moving_averages\n","\n","\n","def plot_reward(data, env_string):\n","    #colors = ['r', 'orange', 'y', 'g', 'c', 'b', 'indigo', 'm', 'grey', 'black']\n","    colors = ['r', 'y', 'c', 'indigo', 'grey', 'grey', 'indigo', 'c', 'y', 'r']\n","    labels = ['(0.01, 1)', '(0.1, 1)', '(1, 1)', '(10, 1)', '(100, 1)', '(1, 0.01)', '(1, 0.1)', '(1, 1)', '(1, 10)', '(1, 100)']\n","\n","    for i in range(len(data)):\n","        plt.plot(np.arange(len(data[i])), data[i], color=colors[i], label=labels[i])\n","\n","    plt.xlabel('# Rounds of 10k steps into Training')\n","    plt.ylabel('Avg Reward calculated every 10k steps')\n","    plt.title(f'Reward over Training from first varying (Radial, Tick) reward weighting in {env_string} environment')\n","    plt.grid(True)\n","\n","    # Add labels to lines\n","    labelLines(plt.gca().get_lines(), zorder=2.5)\n","\n","    plt.show()\n","\n","\n","def plot_rewards_double(data1, data2):\n","    colors = ['r', 'y', 'c', 'indigo', 'grey', 'grey', 'indigo', 'c', 'y', 'r']\n","    labels = ['(0.01, 1)', '(0.1, 1)', '(1, 1)', '(10, 1)', '(100, 1)', '(1, 0.01)', '(1, 0.1)', '(1, 1)', '(1, 10)', '(1, 100)']\n","\n","    plt.figure(figsize=(10, 10))\n","\n","    for i in range(len(data1)):\n","        plt.plot(np.arange(len(y[i])), data1[i], color=colors[i], label=labels[i])\n","\n","    for i in range(len(data2)):\n","        plt.plot(np.arange(len(data2[i])), data2y2[i], color=colors[i])\n","\n","    plt.xlabel('# Rounds of 10k steps into Training')\n","    plt.ylabel('Avg Reward calculated every 10k steps')\n","    plt.title('Avg Reward over Training from varying (Radial, Tick) reward weighting in 0.9-1.1 environment')\n","    plt.grid(True)\n","\n","    # Add labels to lines\n","    labelLines(plt.gca().get_lines(), zorder=2.5)\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkJ2SAYFiNHG"},"outputs":[],"source":["# Memory to storing history of transitions\n","class ReplayBuffer:\n","    def __init__(self):\n","        self.actions = []\n","        self.states = []\n","        self.logprobs = []\n","        self.rewards = []\n","        self.state_values = []\n","        self.is_terminals = []\n","\n","    def clear(self):\n","        del self.actions[:]\n","        del self.states[:]\n","        del self.logprobs[:]\n","        del self.rewards[:]\n","        del self.state_values[:]\n","        del self.is_terminals[:]\n","\n","\n","# One model with the Actor *and* Critic networks bundled together\n","class ActorCritic(nn.Module):\n","    def __init__(self, state_dim, action_dim, hidden_dim, has_continuous_action_space, action_std_init):\n","        super(ActorCritic, self).__init__()\n","        # Works now for both continuous and discrete action states\n","        self.has_continuous_action_space = has_continuous_action_space\n","        if has_continuous_action_space:\n","            self.action_dim = action_dim\n","            self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n","\n","        # Actor\n","        if has_continuous_action_space :\n","            # Output range [-1, 1]\n","\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, action_dim))\n","        else:\n","            # Output discrete n = action_dim actions\n","            self.actor = nn.Sequential(\n","                            nn.Linear(state_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, hidden_dim),\n","                            nn.ReLU(),\n","                            nn.Linear(hidden_dim, action_dim),\n","                            nn.Softmax(dim=-1))\n","\n","        # Critic\n","        self.critic = nn.Sequential(\n","                        nn.Linear(state_dim, hidden_dim),\n","                        nn.ReLU(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.ReLU(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.ReLU(),\n","                        nn.Linear(hidden_dim, hidden_dim),\n","                        nn.ReLU(),\n","                        nn.Linear(hidden_dim, 1))\n","\n","\n","    def set_action_std(self, new_action_std):\n","        if self.has_continuous_action_space:\n","            self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n","        else:\n","          print(\"WARNING : Called ActorCritic::set_action_std() on discrete action space\")\n","\n","\n","    def act(self, state):\n","        if self.has_continuous_action_space:\n","            action_mean = self.actor(state) # mean of action predicted by actor\n","            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n","             # generate distribution to draw actions from based on learned uncertainty in action\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","\n","        action = dist.sample() # get action from distribution (adds randomness)\n","        action_logprob = dist.log_prob(action) # get prob of the action which was drawn for adjusting actor weights\n","        state_val = self.critic(state) # expected critic network value of state for adjusting critic weights\n","\n","        return action.detach(), action_logprob.detach(), state_val.detach()\n","\n","\n","    def evaluate(self, state, action):\n","        if self.has_continuous_action_space:\n","            action_mean = self.actor(state)\n","            action_var = self.action_var.expand_as(action_mean)\n","            cov_mat = torch.diag_embed(action_var)\n","            dist = MultivariateNormal(action_mean, cov_mat)\n","            if self.action_dim == 1:\n","                action = action.reshape(-1, self.action_dim)\n","        else:\n","            action_probs = self.actor(state)\n","            dist = Categorical(action_probs)\n","\n","        action_logprobs = dist.log_prob(action)\n","        dist_entropy = dist.entropy()\n","        state_values = self.critic(state)\n","\n","        return action_logprobs, state_values, dist_entropy\n","\n","\n","class PPO:\n","    def __init__(self, state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma, epochs, eps_clip, has_continuous_action_space, action_std_init=0.6):\n","        self.has_continuous_action_space = has_continuous_action_space\n","        if has_continuous_action_space:\n","            self.action_std = action_std_init\n","\n","        self.gamma = gamma # discount of past reward\n","        self.eps_clip = eps_clip # for clipping values to prevent math issues\n","        self.epochs = epochs\n","        self.memory = ReplayBuffer()\n","        self.policy = ActorCritic(state_dim, action_dim, hidden_dim, has_continuous_action_space, action_std_init)\n","        self.optimizer = torch.optim.Adam([\n","                        {'params': self.policy.actor.parameters(), 'lr': lr_actor, 'betas': (0.99, 0.99)},\n","                        {'params': self.policy.critic.parameters(), 'lr': lr_critic, 'betas': (0.99, 0.99)}\n","                    ])\n","        self.policy_old = ActorCritic(state_dim, action_dim, hidden_dim, has_continuous_action_space, action_std_init)\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        self.MseLoss = nn.MSELoss()\n","\n","\n","    def set_action_std(self, new_action_std):\n","\n","        if self.has_continuous_action_space:\n","            self.action_std = new_action_std\n","            self.policy.set_action_std(new_action_std)\n","            self.policy_old.set_action_std(new_action_std)\n","        else:\n","            print(\"WARNING : Called PPO::set_action_std() on discrete action space\")\n","\n","\n","    def decay_action_std(self, action_std_decay_rate, min_action_std):\n","\n","        if self.has_continuous_action_space:\n","            self.action_std = self.action_std - action_std_decay_rate\n","            self.action_std = round(self.action_std, 4)\n","            if (self.action_std <= min_action_std):\n","                self.action_std = min_action_std\n","            self.set_action_std(self.action_std)\n","\n","        else:\n","            print(\"WARNING : Called PPO::decay_action_std() on discrete action space\")\n","\n","\n","    def select_action(self, state, save):\n","\n","        if self.has_continuous_action_space:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state)\n","                action, action_logprob, state_val = self.policy_old.act(state)\n","\n","            if save:\n","                self.memory.states.append(state)\n","                self.memory.actions.append(action)\n","                self.memory.logprobs.append(action_logprob)\n","                self.memory.state_values.append(state_val)\n","\n","            return action.detach().cpu().numpy().flatten()\n","\n","        else:\n","            with torch.no_grad():\n","                state = torch.FloatTensor(state)\n","                action, action_logprob, state_val = self.policy_old.act(state)\n","\n","            if save:\n","                self.memory.states.append(state)\n","                self.memory.actions.append(action)\n","                self.memory.logprobs.append(action_logprob)\n","                self.memory.state_values.append(state_val)\n","\n","            return action.item()\n","\n","\n","    def update(self):\n","        # Construct Discounted Sum of Rewards\n","        rewards = []\n","        discounted_reward = 0\n","        for reward, is_terminal in zip(reversed(self.memory.rewards), reversed(self.memory.is_terminals)):\n","            if is_terminal:\n","                discounted_reward = 0\n","            discounted_reward = reward + (self.gamma * discounted_reward)\n","            rewards.insert(0, discounted_reward)\n","\n","        # Normalizing Rewards\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n","\n","        # List -> Tensor\n","        old_states = torch.squeeze(torch.stack(self.memory.states, dim=0)).detach()\n","        old_actions = torch.squeeze(torch.stack(self.memory.actions, dim=0)).detach()\n","        old_logprobs = torch.squeeze(torch.stack(self.memory.logprobs, dim=0)).detach()\n","        old_state_values = torch.squeeze(torch.stack(self.memory.state_values, dim=0)).detach()\n","\n","        advantages = rewards.detach() - old_state_values.detach()\n","\n","        for k in range(self.epochs):\n","            print(f\"\\rtraining epoch: {k}\", end=\"\")\n","\n","            # Evaluate old actions and values\n","            logprobs, state_values, dist_entropy = self.policy.evaluate(old_states, old_actions)\n","            state_values = torch.squeeze(state_values)\n","\n","            # Use PPO conservative weight adjustments\n","            ratios = torch.exp(logprobs - old_logprobs.detach())\n","            surr1 = ratios * advantages\n","            surr2 = torch.clamp(ratios, 1-self.eps_clip, 1+self.eps_clip) * advantages\n","            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n","\n","            self.optimizer.zero_grad()\n","            loss.mean().backward()\n","            print(f\"\\t{round(loss.mean().item(), 5)}\", end=\"\")\n","            self.optimizer.step()\n","\n","        self.policy_old.load_state_dict(self.policy.state_dict())\n","        self.memory.clear()\n","\n","    def save(self, checkpoint_path):\n","        torch.save(self.policy_old.state_dict(), checkpoint_path)\n","\n","    def load(self, checkpoint_path):\n","        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n","        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"]},{"cell_type":"markdown","metadata":{"id":"tJL0Gz1PigxI"},"source":["### Navigation to r=1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ZVynXqXi35B"},"outputs":[],"source":["### STARTER ROUTE\n","def varied_l() -> Callable[[], List[np.float32]]:\n","    def func():\n","        r = 1.1\n","        theta = 0\n","        rdot = 0\n","        thetadot = 5.443\n","        pos = [r, 0]\n","        vel = [rdot, r * thetadot]\n","        rot_mat = np.array([[np.cos(theta), -np.sin(theta)],\n","                            [np.sin(theta), np.cos(theta)]])\n","        return [*(rot_mat @ pos), *(rot_mat @ vel)]\n","    return func\n","\n","\n","def make(name):\n","    init_func = varied_l()\n","    if name == 'RocketCircularization-v1':\n","        return RocketEnv(max_step=100, simulation_step=1, rmax=1.6, rmin=0.4, max_thrust=10,\n","                         init_func=init_func,\n","                         oob_penalty=0, dt=0.01, wall_mechanics=False, clip_thrust='None',\n","                         velocity_penalty_rate=0.0, thrust_penalty_rate=0.0)\n","    else: raise ValueError(f'No environment {name}')"]},{"cell_type":"code","source":["env3 = make('RocketCircularization-v1')\n","env3 = CTangentialThrust(PolarizeObservation(env3))\n","\n","state = env3.reset()\n","done = False\n","i = 1\n","while not done:\n","    env3.render()\n","    state, reward, done, truncated, info = env3.step(0)\n","    done = done or truncated\n","    i += 1\n","\n","env3.show(path='test.mp4')\n","show_video('test.mp4')"],"metadata":{"id":"Gd2j7iKtmtEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1J_5sMovY_Z","outputId":"a32a3ded-3635-4d99-d107-c112ea230abd","executionInfo":{"status":"ok","timestamp":1719861290209,"user_tz":240,"elapsed":2054013,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"}}},"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/spaces/box.py:128: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n","  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["training environment name : RocketCircularization-v1\n","current logging run number for RocketCircularization-v1 :  0\n","logging at : PPO_logs/RocketCircularization-v1//PPO_RocketCircularization-v1_log_0.csv\n","save checkpoint path : PPO_preTrained/RocketCircularization-v1/PPO_RocketCircularization-v1_0_0.pth\n","Started training at (GMT) :  2024-07-01 16:23:00\n","============================================================================================\n","training epoch: 79\t0.49035\tEpisode : 3093 \t\t Std: 0.6 \t\t Training Timestep : 50000 \t\t Average Steps Survived : 16.17\n","training epoch: 79\t0.46855\tEpisode : 5004 \t\t Std: 0.6 \t\t Training Timestep : 100000 \t\t Average Steps Survived : 36.8\n","training epoch: 79\t0.48401\tEpisode : 6589 \t\t Std: 0.5 \t\t Training Timestep : 150000 \t\t Average Steps Survived : 254.12\n","training epoch: 79\t0.49464\tEpisode : 7912 \t\t Std: 0.5 \t\t Training Timestep : 200000 \t\t Average Steps Survived : 547.8\n","training epoch: 79\t0.50347\tEpisode : 9095 \t\t Std: 0.4 \t\t Training Timestep : 250000 \t\t Average Steps Survived : 950.49\n","training epoch: 79\t0.48739\tEpisode : 10162 \t\t Std: 0.4 \t\t Training Timestep : 300000 \t\t Average Steps Survived : 1513.08\n","training epoch: 79\t0.48684\tEpisode : 11239 \t\t Std: 0.4 \t\t Training Timestep : 350000 \t\t Average Steps Survived : 1247.88\n","training epoch: 79\t0.49523\tEpisode : 12336 \t\t Std: 0.3 \t\t Training Timestep : 400000 \t\t Average Steps Survived : 1489.15\n","training epoch: 79\t0.49945\tEpisode : 13407 \t\t Std: 0.3 \t\t Training Timestep : 450000 \t\t Average Steps Survived : 1805.71\n","training epoch: 79\t0.49266\tEpisode : 14465 \t\t Std: 0.2 \t\t Training Timestep : 500000 \t\t Average Steps Survived : 2279.62\n","training epoch: 79\t0.49633\tEpisode : 15482 \t\t Std: 0.2 \t\t Training Timestep : 550000 \t\t Average Steps Survived : 2793.5\n","training epoch: 79\t0.494\tEpisode : 16509 \t\t Std: 0.1 \t\t Training Timestep : 600000 \t\t Average Steps Survived : 2427.53\n","training epoch: 79\t0.50639\tEpisode : 17510 \t\t Std: 0.1 \t\t Training Timestep : 650000 \t\t Average Steps Survived : 4657.66\n","training epoch: 79\t0.50554\tEpisode : 18511 \t\t Std: 0.1 \t\t Training Timestep : 700000 \t\t Average Steps Survived : 4677.35\n","training epoch: 79\t0.51282\tEpisode : 19511 \t\t Std: 0.05 \t\t Training Timestep : 750000 \t\t Average Steps Survived : 6528.72\n","training epoch: 79\t0.51542\tEpisode : 20511 \t\t Std: 0.05 \t\t Training Timestep : 800000 \t\t Average Steps Survived : 8187.11\n","training epoch: 79\t0.51151\tEpisode : 21511 \t\t Std: 0.05 \t\t Training Timestep : 850000 \t\t Average Steps Survived : 8316.79\n","training epoch: 79\t0.51184\tEpisode : 22511 \t\t Std: 0.05 \t\t Training Timestep : 900000 \t\t Average Steps Survived : 8207.97\n","training epoch: 79\t0.51427\tEpisode : 23512 \t\t Std: 0.05 \t\t Training Timestep : 950000 \t\t Average Steps Survived : 8427.74\n","training epoch: 79\t0.51282\tEpisode : 24512 \t\t Std: 0.05 \t\t Training Timestep : 1000000 \t\t Average Steps Survived : 8098.73\n","Step: 50 \t [28/50000]============================================================================================\n","Finished training at (GMT) :  2024-07-01 19:14:49\n","Total training time  :  2:51:49\n","============================================================================================\n"]}],"source":["####### setup #######\n","has_continuous_action_space = True  # continuous action space; else discrete\n","max_ep_len = 10_000                 # max timesteps in one episode\n","max_training_timesteps = 1_000_000  # break training loop if timeteps > max_training_timesteps\n","print_freq = max_ep_len * 5         # how often (in timesteps) to print avg reward\n","log_freq = max_ep_len * 5           # how often (in timesteps) to print log avg reward\n","save_model_freq = max_ep_len * 15   # how often (in timesteps) to save model\n","update_timestep = max_ep_len        # how often (in timesteps) to update policy\n","\n","epochs = 80                         # how many (in epochs) to run on updating PPO\n","eps_clip = 0.2                      # parameter for value clipping\n","\n","action_std = 0.6                    # starting std for action distribution (Multivariate Normal)\n","action_std_decay_rate = 0.1        # action_std = action_std - action_std_decay_rate\n","min_action_std = 0.05               # minimum action_std\n","action_std_decay_freq = 120_000\n","                                    # how often (in timesteps) to decay action_std\n","gamma = 0.9996                      # discount factor for considering past summed rewards in value of current state\n","lr_actor = 5e-4                     # lr for actor\n","lr_critic = 5e-4                    # lr for critic\n","hidden_dim = 48                     # hidden layer dimension\n","\n","random_seed = 0                     # seed (can be set to random number, currently set to 0)\n","\n","env_name = 'RocketCircularization-v1' # env name to save model under\n","print(\"training environment name : \" + env_name)\n","\n","env = make(env_name)\n","env.max_step = max_ep_len\n","env.seed()\n","\n","env = PolarizeObservation(env)\n","env = CTangentialThrust(env)\n","\n","state_dim = 4                              # state dimension\n","action_dim = 1                             # action dimension\n","\n","####### logging #######\n","# don't overwrite old log files\n","log_dir = \"PPO_logs\"\n","if not os.path.exists(log_dir):\n","      os.makedirs(log_dir)\n","log_dir = log_dir + '/' + env_name + '/'\n","if not os.path.exists(log_dir):\n","      os.makedirs(log_dir)\n","\n","# num of log files in log directory\n","run_num = 0\n","current_num_files = next(os.walk(log_dir))[2]\n","run_num = len(current_num_files)\n","\n","# new log file for current run\n","log_f_name = log_dir + '/PPO_' + env_name + \"_log_\" + str(run_num) + \".csv\"\n","print(\"current logging run number for \" + env_name + \" : \", run_num)\n","print(\"logging at : \" + log_f_name)\n","\n","\n","####### checkpointing #######\n","# don't overwrite old weights\n","run_num_pretrained = 0\n","\n","directory = \"PPO_preTrained\"\n","if not os.path.exists(directory):\n","      os.makedirs(directory)\n","directory = directory + '/' + env_name + '/'\n","if not os.path.exists(directory):\n","      os.makedirs(directory)\n","\n","checkpoint_path = directory + \"PPO_{}_{}_{}.pth\".format(env_name, random_seed, run_num_pretrained)\n","print(\"save checkpoint path : \" + checkpoint_path)\n","\n","if random_seed:\n","    torch.manual_seed(random_seed)\n","    env.seed(random_seed)\n","    np.random.seed(random_seed)\n","\n","####### training procedure #######\n","ppo_agent = PPO(state_dim, action_dim, hidden_dim, lr_actor, lr_critic, gamma, epochs, eps_clip, has_continuous_action_space, action_std)\n","start_time = datetime.now().replace(microsecond=0)\n","\n","print(\"Started training at (GMT) : \", start_time)\n","print(\"============================================================================================\")\n","\n","# log episode, timestep, reward throughout trainking\n","log_f = open(log_f_name,\"w+\")\n","log_f.write('episode,timestep,reward\\n')\n","print_running_reward = 0\n","print_running_episodes = 0\n","log_running_reward = 0\n","log_running_episodes = 0\n","\n","# training loop\n","rewards = []\n","\n","time_step = 0\n","i_episode = 0\n","while time_step <= max_training_timesteps:\n","    state = env.reset()\n","    state = np.append(state, 0)\n","    env.seed()\n","\n","    current_ep_reward = 0\n","    for t in range(1, max_ep_len+1):\n","        print(f\"\\rStep: {t} \\t [{time_step % print_freq}/{print_freq}]\", end=\"\")\n","\n","        # select action\n","        env.render()\n","        action = ppo_agent.select_action(state, True)\n","        state, reward, done, truncated, info = env.step(100*action[0])\n","        state = np.append(state, t)\n","        done = done or truncated\n","\n","        # no actions after 100 steps, big reward awarded based on how close to the optimal radius and for how long after that point\n","        if t == 50:\n","          while not done:\n","            _, _, done2, truncated2, _ = env.step(0)\n","            done = done2 or truncated2\n","            # radius needs to be updated during loop\n","            reward += (1 # Survival credit - need to look at magnitude given **3\n","                       + 1/(0.01+np.abs(radius - 1)**3) # 240703 Added epsilon\n","            )\n","            current_ep_reward += 1\n","        else:\n","          scale_of_thrust = np.linalg.norm(action, ord=1)\n","          radius = state[0]\n","          reward = 1 + (0.1 * 1/np.abs(radius - 1)) # Added distance from radius\n","\n","        # register new step\n","        ppo_agent.memory.rewards.append(reward)\n","        ppo_agent.memory.is_terminals.append(done)\n","        time_step +=1\n","        current_ep_reward += 1\n","\n","        print_avg_reward = print_running_reward / print_running_episodes if print_running_episodes != 0 else 0\n","        print_avg_reward = round(print_avg_reward, 2)\n","\n","        # update PPO agent\n","        if time_step % update_timestep == 0:\n","            ppo_agent.update()\n","\n","        # decay action_std when necessary\n","        if has_continuous_action_space and time_step % action_std_decay_freq == 0:\n","            ppo_agent.decay_action_std(action_std_decay_rate, min_action_std)\n","\n","        # log in logging file\n","        if time_step % log_freq == 0:\n","            log_avg_reward = log_running_reward / log_running_episodes\n","            log_avg_reward = round(log_avg_reward, 4)\n","            log_f.write('{},{},{}\\n'.format(i_episode, time_step, log_avg_reward))\n","            log_f.flush()\n","            log_running_reward = 0\n","            log_running_episodes = 0\n","\n","        # print avg reward\n","        if time_step % print_freq == 0:\n","            print(\"\\tEpisode : {} \\t\\t Std: {} \\t\\t Training Timestep : {} \\t\\t Average Steps Survived : {}\".format(i_episode, ppo_agent.action_std, time_step, print_avg_reward))\n","            print_running_reward = 0\n","            print_running_episodes = 0\n","\n","        # # save weights\n","        # if time_step % log_freq == 0 and print_avg_reward == max_ep_len:\n","        #     print(\"--------------------------------------------------------------------------------------------\")\n","        #     print(\"saving model at : \" + checkpoint_path)\n","        #     ppo_agent.save(checkpoint_path)\n","        #     time_step = max_training_timesteps\n","        #     print(\"model saved\")\n","        #     print(\"Elapsed Time  : \", datetime.now().replace(microsecond=0) - start_time)\n","        #     print(\"--------------------------------------------------------------------------------------------\")\n","\n","        if done:\n","            rewards.append(current_ep_reward)\n","            break\n","\n","    print_running_reward += current_ep_reward\n","    print_running_episodes += 1\n","    log_running_reward += current_ep_reward\n","    log_running_episodes += 1\n","    i_episode += 1\n","\n","log_f.close()\n","env.close()\n","\n","# print total training time\n","print(\"============================================================================================\")\n","ppo_agent.save(checkpoint_path)\n","end_time = datetime.now().replace(microsecond=0)\n","print(\"Finished training at (GMT) : \", end_time)\n","print(\"Total training time  : \", end_time - start_time)\n","print(\"============================================================================================\")"]},{"cell_type":"markdown","source":["#### Results"],"metadata":{"id":"gpbq-YY2fdNu"}},{"cell_type":"code","source":["state = env.reset()\n","state = np.append(state, 0)\n","done = False\n","i = 1\n","while not done:\n","    env.render()\n","    if i <= 50:\n","        action = ppo_agent.select_action(state, True)\n","    else:\n","        action = [0]\n","    state, reward, done, truncated, info = env.step(100*action[0])\n","    state = np.append(state, i)\n","    done = done or truncated\n","    i += 1\n","print(i)\n","\n","env.show(path='test.mp4')\n","show_video('test.mp4')"],"metadata":{"id":"kaijfXKOG5e2","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"11nmzWBBSVpnm8A9lLFR01AP51H_CuBy2"},"executionInfo":{"status":"ok","timestamp":1719865247339,"user_tz":240,"elapsed":3957132,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"}},"outputId":"6b9b251c-c5ca-4946-f5e2-019e8c0b2672"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","source":["state = env.reset()\n","state = np.append(state, 0)\n","done = False\n","i = 1\n","while not done:\n","    env.render()\n","    if i <= 50:\n","        action = ppo_agent.select_action(state, True)\n","    else:\n","        action = [0]\n","    state, reward, done, truncated, info = env.step(100*action[0])\n","    state = np.append(state, i)\n","    done = done or truncated\n","    i += 1\n","print(i)\n","\n","env.show(path='test.mp4')\n","show_video('test.mp4')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kwBrl0mzi9wnM7v-zBOoKjbnMDZc58ZN"},"id":"4jC9ArGIao1q","executionInfo":{"status":"ok","timestamp":1719817444937,"user_tz":240,"elapsed":3766128,"user":{"displayName":"Benjamin Bradley","userId":"04603304923366878970"}},"outputId":"7ffb7dab-eaf7-4371-8e16-03c62cf1a3ba"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["lCHMHxhtharQ"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}